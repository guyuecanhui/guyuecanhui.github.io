<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HCigmoid</title>
  
  <subtitle>Watch, learn and practise</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://guyuecanhui.github.io/"/>
  <updated>2020-05-09T16:09:58.581Z</updated>
  <id>https://guyuecanhui.github.io/</id>
  
  <author>
    <name>古月残辉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AutoInt 论文精读</title>
    <link href="https://guyuecanhui.github.io/2020/05/09/paper-2019-pku-autoint/"/>
    <id>https://guyuecanhui.github.io/2020/05/09/paper-2019-pku-autoint/</id>
    <published>2020-05-09T15:46:32.000Z</published>
    <updated>2020-05-09T16:09:58.581Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>论文引用:</strong> Song, Weiping, et al. “Autoint: Automatic feature interaction learning via self-attentive neural networks.” <em>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em>. 2019.</p></blockquote><p>这是一篇北京大学发表在 <strong>CIKM 2019</strong> 的文章，看作者列表没有企业背景，主要还是提供一些理论思路。文章的核心也是想通过自动挖掘特征间的高阶交互关系来提升减少人工特征工程，但是与前面的 <strong>DeepFM</strong>、<strong>DCN</strong> 等能够提供显式特征交叉能力的模型最大的差别在于：本文是通过不同 field 间特征做 <strong>Self-Attention</strong> 来实现特征的交互，也因此获得了一定的特征组合的可视化能力（<em>即文章中声称提供了较好的可解释性</em>）。</p><h3 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h3><p>其实已有的深度模型的相关工作基本核心都是在做高阶的特征交叉，但是诸如 <strong>PNN</strong>、<strong>FNN</strong>、<strong>DeepCrossing</strong>、<strong>Wide&amp;Deep</strong>、<strong>DeepFM</strong> 等模型，主要是依赖前馈网络来实现高阶特征交叉，主要的问题是特征交叉过程是隐式的，很难解释是哪些特征间的组合起到了关键作用，这个问题也存在于 <strong>DCN</strong>、<strong>xDeepFM</strong> 等提供显式特征交叉能力的模型中。</p><p>另外一些提供显式特征交叉能力的模型和算法也存在各种各样的问题，比如基于树+embedding 的$^{[1,2,3]}$，会将训练过程分裂成几个部分；比如显式做所有特征的高阶组合的 <strong>HOFM</strong> 模型$^{[4]}$，参数量过大，高于 5 阶的组合基本不可用（<em>实际上根据张俊林的说法，高于 4 阶的特征组合就已经收益很低了，这在 <strong>DCN</strong> 的测试中也得到了一定的验证</em>）。</p><p>基于这个背景，文章的目标是想找到一种特征自动进行高阶交叉的方法，既能弥补 <strong>MLP</strong> 对乘性特征组合捕获能力不强的弱点，又能够较好的解释哪些特征组合比较有效。</p><h3 id="AutoInt-网络设计"><a href="#AutoInt-网络设计" class="headerlink" title="AutoInt 网络设计"></a>AutoInt 网络设计</h3><h4 id="模型概览"><a href="#模型概览" class="headerlink" title="模型概览"></a>模型概览</h4><p><strong>AutoInt</strong> 网络内部主要包括两块，如下图所示：</p><img src="/2020/05/09/paper-2019-pku-autoint/autoint.png" title="AutoInt 整体架构"><p><strong>AutoInt</strong> 底层是 embedding 层，类似于 <strong>DeepFM</strong> 的设计，将所有的离散、连续特征都映射成一个等长的 embedding 向量，其中，离散特征是直接 lookup embedding 表，多值的离散特征使用 average pooling；连续特征则相当于乘以一个不含 bias 的 <strong>Dense</strong> 层。</p><p><strong>AutoInt</strong> 核心是上面的交互层，使用 <strong>Multi-head Self-Attention</strong> 来实现，并且可以叠加多层，实现特征的高阶交叉。作者认为，特征组合的关键是知道哪些特征组合在一起有强大的表征能力，这个实际上相当于在人工特征工程中进行特征选择，那么在深度网络里怎么自动去实现特征组合的选择呢？作者受到 <strong>Self-Attention</strong> 的启发，考虑让每个 field 的特征与其他 field 的特征分别做 attention，根据 attention 的权重来判断该 field 特征与其他 field 特征组合的重要性，越重要的组合给予的权重越高，最后生成加权后的 sum pooling 作为该 field 特征与所有其他 field 特征组合的结果。</p><h4 id="Self-Attention-层计算流程详解"><a href="#Self-Attention-层计算流程详解" class="headerlink" title="Self-Attention 层计算流程详解"></a>Self-Attention 层计算流程详解</h4><p>关于 <strong>Self-Attention</strong> 相关的理论和应用，后面还会再单独介绍，感觉这一块是后面网络发展的重点。下面仅仅结合计算过程再详细的说明一下上面的思路，使用的符号与文章略有不同。</p><ol><li>假设输入特征一共有 $m$ 个 field，每个 field 的特征记为 $\boldsymbol{x}_i$，对应的 embedding 向量记为 $\boldsymbol{e}_i$；所有 field 拼接起来的 embedding 记为 $\boldsymbol{e}$；</li><li>考虑第 $i$ 个 field 的特征 $\boldsymbol{x}_i$ 对应的 embedding 向量 $\boldsymbol{e}_i$，首先计算它经过单层 Self-Attention 后生成的特征组合向量 $\tilde{\boldsymbol{e}}_i$（<em>其他 field 的计算过程完全相同</em>）；</li><li>对于 head $h$，根据该 head 中的 query、key、value 矩阵，计算第 $i$ 个 field 特征的 query 向量和所有其他 field 特征的 key、value 向量：<script type="math/tex; mode=display">\begin{cases}\boldsymbol{q}_i^{(h)} = \boldsymbol{W}_q^{(h)}\boldsymbol{e}_i \\\boldsymbol{k}_j^{(h)} = \boldsymbol{W}_k^{(h)}\boldsymbol{e}_j \\\boldsymbol{v}_j^{(h)} = \boldsymbol{W}_v^{(h)}\boldsymbol{e}_j\end{cases} \qquad j=1,\cdots,m</script></li><li>计算第 $i$ 个 field 的 query 向量与所有其他 field 特征对应的 key 向量的 attention 权重 <script type="math/tex">a_{i,j}^{(h)}</script>：<script type="math/tex; mode=display">a_{i,j}^{(h)} = \frac{\exp(\boldsymbol{q}_i^{(h)}\cdot\boldsymbol{k}_j^{(h)})}{\sum_{l=1}^m \exp(\boldsymbol{q}_i^{(h)}\cdot \boldsymbol{k}_l^{(h)})}</script></li><li>计算第 $i$ 个 field 对应的所有其他 field 的加权 value 向量 $\tilde{\boldsymbol{e}}_i^{(h)}$，作为第 $h$ 个 head 中第 $i$ 个 field 特征与所有其他 field 特征交互后的组合向量：<script type="math/tex; mode=display">\tilde{\boldsymbol{e}}_i^{(h)}=\sum_{j=1}^m a_{i,j}^{(h)} \boldsymbol{v}_j^{(h)}</script></li><li>将所有 head 的结果进行 concat，得到第 $i$ 个 field 特征与所有其他 field 特征交互后的组合向量：<script type="math/tex; mode=display">\tilde{\boldsymbol{e}}_i = \tilde{\boldsymbol{e}}_i^{(1)}\oplus\tilde{\boldsymbol{e}}_i^{(2)}\oplus\cdots\oplus\tilde{\boldsymbol{e}}_i^{(k)}</script></li><li>将所有 field 的组合向量进行 concat，并加上输入层，得到单层 Self-Attention 的输出向量 $\tilde{\boldsymbol{e}}$：<script type="math/tex; mode=display">\tilde{\boldsymbol{e}}' = \tilde{\boldsymbol{e}}_1 \oplus\tilde{\boldsymbol{e}}_2\oplus\cdots\oplus\tilde{\boldsymbol{e}}_m \\\tilde{\boldsymbol{e}}=\text{ReLU}(\tilde{\boldsymbol{e}}'+\boldsymbol{W}\cdot \boldsymbol{e})</script></li></ol><p><strong>说明：</strong></p><ul><li>与 google 的原始论文 $[5]$ 相比，<strong>AutoInt</strong> 中的 <strong>Self-Attention</strong> 没有进行 scale，即第 4 步求 softmax 之前没有将内积除以一个缩放系数，导致的结果是突出了高效组合的重要性。当然，在实现的时候还是可以尝试把缩放加进来；</li><li>文献 $[5]$ 在最后一步还会再过一个 <strong>LayerNormalization</strong>，文章里并没有加；实现的时候可以加了看看效果；</li><li>在实现的时候，假设 embedding 的维度为 $d$，head 的数量为 $k$，则可以设置每个 head 中 query、key、value 矩阵 $\boldsymbol{W}^{(h)}$ 的维度为 $(\frac{d}{k}, d)$，这样得到的 $\tilde{\boldsymbol{e}}_i^{(h)}$ 就是 $\frac{d}{k}$ 维，将 $k$ 个 head 的结果 concat 后，$\tilde{\boldsymbol{e}}_i$ 又变成了 $d$ 维，从而保证输入的维度与输出的维度相同；这样的话，最后一步中矩阵 $\boldsymbol{W}$ 其实就不需要了（<em>文章的实验中 $\boldsymbol{e}$ 与 $\tilde{\boldsymbol{e}}’$ 维度不同，因此需要通过 $\boldsymbol{W}$ 将 $\boldsymbol{e}$ 变成与 $\tilde{\boldsymbol{e}}’$ 相同的维度</em>）。</li></ul><h4 id="Self-Attention-交叉能力分析"><a href="#Self-Attention-交叉能力分析" class="headerlink" title="Self-Attention 交叉能力分析"></a>Self-Attention 交叉能力分析</h4><p>文章将有 $p$ 个不同 field 特征乘性组合的特征称为 $p$ 阶组合特征，记为 $g(x<em>{i_1},\cdots,x</em>{i_p})$，从计算过程容易看出来，$\tilde{\boldsymbol{e}}_i^{(h)}$ 乃至 $\tilde{\boldsymbol{e}}_i$ 都是包含 $\boldsymbol{x}_i$ 与所有 $\boldsymbol{x}_j\ (j=1,\cdots,i-1,i+1,\cdots,m )$ 交互的 2 阶组合特征：${g(x_i,x_1),g(x_i,x_2),\cdots,g(x_i,x_m)}$。因此，单层 Self-Attention 就能表征所有 field 的 2 阶组合特征。</p><p>到了两层时，由于第一层输出中每个 field 相当于都是包含了所有的 2 阶组合，因此它的输出就包含了 3 阶和 4 阶的组合特征，例如 $g(x_1,x_2,x_3,x_4)$ 就包含在 $\tilde{\boldsymbol{e}}_1$ 和 $\tilde{\boldsymbol{e}}_3$ 的交互中。同理，三层 <strong>Self-Attention</strong> 就包含 8 阶内的组合特征……与 <strong>DCN</strong> 中的 cross 层相比，cross 每层增加 1 阶特征组合，而 <strong>Self-Attention</strong> 每层增加 1 倍特征组合。</p><h4 id="应用与讨论"><a href="#应用与讨论" class="headerlink" title="应用与讨论"></a>应用与讨论</h4><p>上面已经介绍了 embedding 层和 <strong>Self-Attetion</strong> 层，其中，<strong>Self-Attention</strong> 层是可以直接堆叠的，由于有残差结构的设计（最后一步加上了输入），理论上可以堆的比较深（文章的实验也证明了这个设计是比较有效的）。它还可以作为子结构，通过串联或者并联的方式，嵌入到其他网络中去，例如：</p><ul><li><strong>串联：</strong>最上面一层的 <strong>Self-Attention</strong> 输出可以直接送到 <strong>LR</strong> 里输出预测结果，或者再接一个 <strong>MLP</strong> 再输出预测结果；</li><li><strong>并联：</strong>embedding 层同时作为输入，送到 <strong>MLP</strong>、<strong>FM</strong>、cross 等其他层中，最后所有层结果进行 concat，送到 <strong>LR</strong> 中输出预测结果；</li></ul><p>训练一般还是使用 logloss 作为损失函数，用 <strong>Adam</strong> 等优化算法进行优化。</p><p>我在我们的数据集上测试的时候，发现 <strong>Self-Attention</strong> 层数也是 3 层就够了，到了 4 层测试 <strong>AUC</strong> 反而会降低，这与文章的参数是吻合的。</p><p>至于文章另外一个鼓吹的亮点，即特征组合的可解释性，实际上就是画出 attention 权重的热力图，主要是用于数据分析，感觉除了汇报好看点，也没啥实际的用处。</p><p>最后想说的一点，文章将不同 field 的特征当成了序列来做 <strong>Self-Attention</strong>，但其实 <strong>Self-Attention</strong> 也经常会用于对序列特征做 pooling，这也会在以后一起介绍。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Wang, Xiang, et al. “Tem: Tree-enhanced embedding model for explainable recommendation.” <em>Proceedings of the 2018 World Wide Web Conference</em>. 2018.<br>[2] Zhao, Qian, Yue Shi, and Liangjie Hong. “Gb-cent: Gradient boosted categorical embedding and numerical trees.” <em>Proceedings of the 26th International Conference on World Wide Web</em>. 2017.<br>[3] Zhu, Jie, et al. “Deep embedding forest: Forest-based serving with deep embedding features.” <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. 2017.<br>[4] Blondel, Mathieu, et al. “Higher-order factorization machines.” <em>Advances in Neural Information Processing Systems</em>. 2016.<br>[5] Vaswani, Ashish, et al. “Attention is all you need.” <em>Advances in neural information processing systems</em>. 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;论文引用:&lt;/strong&gt; Song, Weiping, et al. “Autoint: Automatic feature interaction learning via self-attentive neural netw
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="注意力" scheme="https://guyuecanhui.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="self-attention" scheme="https://guyuecanhui.github.io/tags/self-attention/"/>
    
      <category term="ctr" scheme="https://guyuecanhui.github.io/tags/ctr/"/>
    
  </entry>
  
  <entry>
    <title>DeepFM 论文精读</title>
    <link href="https://guyuecanhui.github.io/2020/04/30/paper-2017-noah-deepfm/"/>
    <id>https://guyuecanhui.github.io/2020/04/30/paper-2017-noah-deepfm/</id>
    <published>2020-04-30T14:30:24.000Z</published>
    <updated>2020-05-25T15:21:24.687Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>论文引用</strong>: Guo, Huifeng, et al. “DeepFM: a factorization-machine based neural network for CTR prediction.” <em>arXiv preprint arXiv:1703.04247</em> (2017).</p></blockquote><p><strong>DeepFM</strong> 是华为诺亚实验室受 <strong>FM</strong> 和 wide &amp; deep 模型启发，发表在 <strong>IJCAI 2017</strong> 的一个 <strong>CTR</strong> 预估模型，从国内企业的实践分享来看，其效果受到了广泛的认可。它的核心思想是将 wide &amp; deep 网络中的 wide 层用 <strong>FM</strong> 层代替，增加了特征的二阶自动交叉能力，并且在实现上天然可以将 <strong>FM</strong> 层的 embedding 与 deep 层共享。</p><h3 id="DeepFM-架构"><a href="#DeepFM-架构" class="headerlink" title="DeepFM 架构"></a>DeepFM 架构</h3><p>随着深度网络的兴起，<strong>DNN</strong> 尤其是 <strong>MLP</strong> 的结构往往被用作非线性的特征提取器，并且取得了比较好的效果。然而文章指出，<strong>只保留特征的高阶非线性组合并不如同时保留特征的低阶组合和高阶组合效果好</strong>。这也是文章核心的出发点之一。但是 wide &amp; deep 模型中只有特征的一阶和高阶组合，因此一些有效的的二阶/三阶组合只能靠工程师人工挖掘组合出来，放到 wide 层。这个时候 <strong>FM</strong> 模型又出场了，用它来代替 wide 层，既能保留原始特征，又能自动做原始特征的二阶组合，这样就能节省很多人工特征交叉的工作。当然对于三阶及以上特征的显式组合，还是只能靠人工做一些工作，或者用前面讲的 <strong>DCN</strong> 中的 cross 等结构去捕获。</p><p><strong>DeepFM</strong> 的模型架构如下图所示，首先将所有的特征转成长度相同的 embedding，然后依照 <strong>FM</strong> 模型构建 <strong>FM</strong> 层，再将所有 embedding 拼接后，送到 <strong>MLP</strong> 层，最后在顶层进行拼接后，送到 <strong>LR</strong> 层，计算二分类损失。</p><img src="/2020/04/30/paper-2017-noah-deepfm/deepfm_architecture.png" title="图 1. DeepFM 整体架构"><p>从架构来看，<strong>DeepFM</strong> 没有什么创新的子结构，主要还是在当时提出了一个比较好的思路。下面再聊一聊模型中的一些细节处理。</p><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>由于诺亚在应用市场推荐场景中没有使用连续特征，因此该模型也没有考虑连续特征的处理。但是实际上 <strong>FM</strong> 本身是支持连续特征的，所以如果有连续特征，应该也是需要将每一维连续特征转成一个 embedding。</p><p>对于多值的离散特征，这里有两种处理方案。一种是只考虑 field 之间的特征交叉，这种思路下，我们在生成多值离散特征的 embedding 时可以使用某种 pooling 策略，建议使用 sum pooling，因为这种情况下相当于是每个取值都与另外一维特征做了交叉；另一种是考虑所有特征取值的交叉，这种思路下，就需要将多值特征每个取值的 embedding 进行 concat，再送到 <strong>FM</strong> 层和 <strong>MLP</strong> 层。</p><h3 id="效果测试"><a href="#效果测试" class="headerlink" title="效果测试"></a>效果测试</h3><p>我测试了一下 <strong>DeepFM</strong> 每个子结构的性能（使用 pooling 策略），以只使用线性部分作为基线的话，只使用 FM 的交叉部分能够提升 $4.91\%$，只使用 <strong>MLP</strong> 部分能够提升 $9.30\%$，全部使用能够提升 $9.38\%$，从数据来看，<strong>FM</strong> 层的确能带来少量的提升。</p><h3 id="从-DeepFM-到-DeepFFM"><a href="#从-DeepFM-到-DeepFFM" class="headerlink" title="从 DeepFM 到 DeepFFM"></a>从 DeepFM 到 DeepFFM</h3><p>最后有一个小问题值得思考和讨论一下，<strong>DeepFM</strong> 使用 <strong>FM</strong> 层代替 wide 层，那能不能使用 <strong>FFM</strong> 来代替 wide 层呢？或者有没有什么类似的思路，让每个特征与其他 field 特征进行交叉的时候能够有不同的表征？</p><p>最直接的方案就是将 embedding 层进行扩展（称为 <strong>NeuralFFM</strong> $^{[1]}$），即将原来每个 field 对应一个二维的 embedding matrix，扩展为每个 field 对应一个三维的 embedding matrix，使得每个特征都对应于一个二维的 embedding matrix，在交叉时同时根据 source/target field ID 来 lookup 相应的 embedding vector，如图 2 所示。假设 field 的数量为 $m$，这种方案实际上是将 embedding 的参数量增加为原来的 $m$ 倍。这种方案貌似最早是南大在比赛中使用过，效果还不错。但是由于 embedding 层本来就已经贡献了整个模型主要的参数量，这种方案无论是在 train、store 还是 serving 都十分吃资源。</p><img src="/2020/04/30/paper-2017-noah-deepfm/neuralffm_architecture.png" title="图 2. NeuralFFM 整体架构"><p>还有一种思路是使用 self-attention 的思路 $^{[2]}$，将每个 field 与其他 field 计算 attention 值，作为其他 field 对该 field 相关的权重，然后再计算加权的 pooling，作为该 field 与其他所有 field 特征的交互结果。这个方案我们在 后续介绍 <a href="https://guyuecanhui.github.io/2020/05/09/paper-2019-pku-autoint/">AutoInt</a> 时会再详细说明。</p><p>张俊林等人则同时采用了这两种思路，提出了 <strong>FAT-DeepFFM</strong> $^{[3]}$，即 embedding 层使用 <strong>FFM</strong> 的扩展，在交互层使用 attention 机制，如下所示。但是张俊林自己在分享中都表示这种思路尝试一下就好了 $^{[4]}$，所以这里就学习一下这种思路，不详细介绍了，大家在生产环境中还是谨慎使用。</p><img src="/2020/04/30/paper-2017-noah-deepfm/deepffm_architecture.png" title="图 3. FAT-DeepFFM 整体架构"><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Yang, Yi, et al.. “Neural field-aware factorization machine.” <em><a href="https://cs.nju.edu.cn/31/60/c1654a209248/page.htm" target="_blank" rel="noopener">https://cs.nju.edu.cn/31/60/c1654a209248/page.htm</a></em>. 2017.</p><p>[2] Song, Weiping, et al. “Autoint: Automatic feature interaction learning via self-attentive neural networks.” <em>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em>. 2019.</p><p>[3] Zhang, Junlin, Tongwen Huang, and Zhiqi Zhang. “FAT-DeepFFM: Field Attentive Deep Field-aware Factorization Machine.” <em>arXiv preprint arXiv:1905.06336</em> (2019).</p><p>[4] 张俊林. “FFM及DeepFFM模型在推荐系统的探索.” <em><a href="https://zhuanlan.zhihu.com/p/67795161" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67795161</a></em>. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;论文引用&lt;/strong&gt;: Guo, Huifeng, et al. “DeepFM: a factorization-machine based neural network for CTR prediction.” &lt;em&gt;a
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="特征组合" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88/"/>
    
      <category term="特征交叉" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89/"/>
    
  </entry>
  
  <entry>
    <title>Deep &amp; Cross 论文精读</title>
    <link href="https://guyuecanhui.github.io/2020/04/15/paper-2017-google-dcn/"/>
    <id>https://guyuecanhui.github.io/2020/04/15/paper-2017-google-dcn/</id>
    <published>2020-04-15T13:50:20.000Z</published>
    <updated>2020-05-09T15:43:27.071Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文引用: Wang, Ruoxi, et al. “Deep &amp; cross network for ad click predictions.” <em>Proceedings of the ADKDD’17</em>. 2017. 1-7.</p></blockquote><p>本文是 Stanford 和 Google 联合发表在 <strong>KDD</strong> 2017 workshop 上的一篇 <strong>CTR</strong> 预估模型，模型采用 <strong>Wide&amp;Deep</strong> 架构，最大的创新点在于将 wide 层替换成 cross 层，省去了原本大量的人工特征工程的工作，由 cross 层提供显式的高阶特征组合能力，同时保持较低的参数量和计算量。</p><h3 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h3><p>随着深度网络的兴起，各大厂纷纷尝试用深度网络来作为特征提取器，并取得了显著的效果。尤其是 Wide&amp;Deep 架构的提出，让大家意识到可以在模型中同时使用 <strong>DNN</strong> 来捕获特征的复杂映射关系，使用线性模型来记忆组合特征，但是这里的线性模型中的组合特征其实还是工程师针对具体场景人工构造的。</p><p>进一步，大家考虑将 wide 层用自动特征组合的网络来替换，例如用 <strong>FM</strong> 来代替 wide 层 (<strong>DeepFM</strong>)，能够提供了二阶的自动特征组合能力。本文则更进一步，设计出 cross 网络来代替 wide 层，能够提供任意高阶的特征组合能力 (与 cross 层数相同)，同时保持很低的计算和存储开销。</p><p>那有了 <strong>MLP</strong> 层为什么还需要 cross 层呢？我的理解是，<strong>MLP</strong> 层更擅长的是进行特征复杂的非线性变换，这些非线性变换通常是隐式的、对于乘性的特征组合捕获能力不强，而人基于对业务的深入理解能够显式构造一些十分有效的组合特征，如果让 <strong>MLP</strong> 层去学，可能需要比较大的参数规模才能学到。这也是 <strong>Wide&amp;Deep</strong> 为什么还需要保留 wide 层的原因。而 cross 层则设计为显式的进行特征组合，能够保证每一层都保留上一层的组合信息，并增加一阶组合，因此可以认为这两个结构是互补的。</p><p>当然，这里的 cross 层由于参数数量的限制，容量有限，我们仍然可以在 <strong>DCN</strong> 的基础上增加 wide 层来强化对部分人工特征的记忆，或者将 cross 层作为一个子网嵌入到其他排序网络中去，作为高阶特征提取器。</p><h3 id="DCN-网络架构"><a href="#DCN-网络架构" class="headerlink" title="DCN 网络架构"></a>DCN 网络架构</h3><p>前面提到了，<strong>DCN</strong> 的主要创新点在于设计了 cross 层来做特征的显式组合，同时只使用很少的参数。乍一看感觉有点神奇，如果要学习 $n$ 个特征的 $k$ 阶组合，按道理讲我们需要 $C_n^k$ 个参数 ($O(d^n)$)，那怎么进行参数的压缩呢？先可以回顾一下 <strong>FM</strong>，它本质上是学习了每个特征的隐向量，然后用两个特征隐向量的内积来代替这两个特征组合的系数，这样能够共享统计强度，并减少参数量。<strong>DCN</strong> 的 cross 层则更进一步，它每一层的设计如下：</p><script type="math/tex; mode=display">\boldsymbol{x}_{l+1}=\boldsymbol{x}_0 \boldsymbol{x}_l^{\top}\boldsymbol{w}_l + \boldsymbol{b}_l + \boldsymbol{x}_l \qquad(1)</script><p>即第 $l+1$ 层的输入与第 $l$ 层的输入和原始的特征输入有关。再具体点来说，式 $(1)$ 右边的第一项是原始特征与 $l$ 阶特征的笛卡尔积，再与一个权重向量相乘。简单用归纳法可以看出，从表达式上来讲，$\boldsymbol{x}_0 \boldsymbol{x}_l$ 是能够包含所有 $l+1$ 阶的特征组合，这一项再乘以一个权重向量，相当于对特征组合的信息做了 weighted sum pooling，或者换句话说，$\boldsymbol{x}_0 \boldsymbol{x}_l^{\top}$ 的每一列 $i$ 共享同一个参数 $\boldsymbol{w}_l^{(i)}$；式 $(1)$ 最后再加上原始特征输入实际上是类似于残差网络的结构，主要是帮助模型训练。关于式 $(1)$ 文章还提供了一个可视化的图片方便理解，如下所示：</p><img src="/2020/04/15/paper-2017-google-dcn/dcn_cross.png" title="图 1. Cross 层结构设计"><p>到这里我们可以简单的分析一下这个 cross 层有以下特点：</p><ol><li>它的显式特征组合是通过输入与每一层进行笛卡尔积来获得，但是在保存的时候，进行了 pooling，极大的降低了存储的开销；</li><li>它每层的权重实际上是一个 $d$ 维的向量 (而非两层全连接网络的 <script type="math/tex">n_i\times n_{i-1}</script> 的张量)，因此参数规模控制在 $O(md)$ 的级别，其中 $m$ 是 cross 网络的层数，$d$ 是初始特征维度；</li><li>实际计算的时候，可以根据交换律先计算 $\boldsymbol{x_0} (\boldsymbol{x}_l^{\top}\boldsymbol{w})$ 中括号内的部分，将乘法次数降到 $O(d)$ 次，从而整个 cross 网络的推理复杂度也降到了 $O(md)$。</li><li>cross 网络不存在非线性的变换，并且由于权重的连乘，在训练的时候存在梯度消失或者爆炸的风险。</li></ol><p>我们可以将 cross 层作为一个子结构加入到其他网络中去，例如 <strong>DCN</strong> 就可以看成是普通的 <strong>MLP</strong> + cross 的架构，其中，<strong>MLP</strong> 部分主要来做特征的非线性变换，一般就是几层全连接的隐含层，使用 <strong>relu</strong> 进行激活。整体的架构如下：</p><img src="/2020/04/15/paper-2017-google-dcn/dcn_architecture.png" title="图 1. DCN 整体架构"><p>在特征处理时，连续特征做归一化，离散特征先查 embedding，拼接在一起作为原始输入 $\boldsymbol{x}_0$，并分别输入到 cross 层和 <strong>MLP</strong> 层，将两个结构的输出拼接后，经过一个 <strong>LR</strong> 输出预测结果，训练的时候使用带 <strong>L2</strong> 范数的 logloss 作为 <strong>Cost Function</strong>，用 <strong>Adam</strong> 之类的优化算法求解。</p><p>对于模型训练，文章还分享了一些小技巧：</p><ol><li><strong>MLP</strong> 层使用了 batch normalization，并设置梯度截断值为 100；</li><li>使用 early stop 来进行正则化，相对于 L2 正则和 dropout 更有效；感觉 <strong>DCN</strong> 的确比较容易过拟合，我在测试时发现，当数据量比较大，并且将模型的参数规模调整到一个合适的量级，一个 epoch 基本已经达到最优了，继续训练可能就会导致测试 <strong>AUC</strong> 下降；</li><li>$d$ 维类别特征的 embedding size 设置参考计算式：$6\times d^{\frac{1}{4}}$；但是感觉在我们的场景里还是太大了，个人倾向于设置成 $\log$ 的函数。在资源允许的情况下，embedding size 设置大一点对测试 <strong>AUC</strong> 是有正向帮助的，但是也增加了过拟合的风险；</li></ol><h3 id="实验与讨论"><a href="#实验与讨论" class="headerlink" title="实验与讨论"></a><strong>实验与讨论</strong></h3><p>文章主要在 <a href="https://www.kaggle.com/c/criteo-display-ad-challenge" target="_blank" rel="noopener">criteo 数据集</a>上进行了一些对比实验，除了证明 <strong>DCN</strong> 比 benchmark 都好很多以外，还重点强调了 <strong>DCN</strong> 在同等参数量下能够达到更低的误差、在同等误差的情况下能够使用更少的参数。另外，还单独测试了增加 cross 结构的确能够大幅的降低模型的误差。</p><p>文章比较的时候，<strong>LR</strong>、<strong>FM</strong> 等模型使用的特征与 <strong>DCN</strong> 实际上是有差别的，做了更多的特征工程。为了测试模型本身的效果，我在我们业务的数据集上也跟一些常见模型对比测试了一下，离线效果确实还不错，而且即使只用 cross 层，也能超过 <strong>FFM</strong> 的效果。</p><p>我也单独测试了 <strong>DCN</strong> 各个子结构的性能，经过若干贪心调优后，以 <strong>LR</strong> 作为基线 (即将 <strong>DCN</strong> 的 <strong>MLP</strong> 层和 cross 层的输入 $\boldsymbol{x_0}$ 直接送到 <strong>LR</strong> 层)，只使用 cross 层的测试 <strong>AUC</strong> 提升了 $4.5\%$，只使用 <strong>MLP</strong> 层的测试 <strong>AUC</strong> 提升了 $5.5\%$，而两者都用的测试 <strong>AUC</strong> 提升了 $6.3\%$。只使用 cross 层效果不如只使用 <strong>MLP</strong> 层的主要原因是参数数量少了很多，即使是 10 层的 cross 层参数量也就相当于一个只含有 20 个神经元的单层全连接 <strong>MLP</strong>。如果设定两者的参数量相同的情况下，在我们的数据集上，只使用 cross 层能够比只使用 <strong>MLP</strong> 层高出 $1‰$。但是 cross 层想要把参数量提上来其实很困难，在测试中，cross 层达到 20 层的时候，训练的时长比 4 层增加 $200\%$，并且 loss 在某个 batch 以后突然爆炸（不影响 <strong>AUC</strong> 评估），因此感觉 cross 层还是只能作为一个辅助的子结构使用。一个可以尝试的方法是同时使用 2 ~ 4 层的 cross 层 (5 层相对于 4 层的收益已经很少了)，在顶层进行 concat，最后送到 <strong>LR</strong> 中去。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;论文引用: Wang, Ruoxi, et al. “Deep &amp;amp; cross network for ad click predictions.” &lt;em&gt;Proceedings of the ADKDD’17&lt;/em&gt;. 2017. 1
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="特征组合" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88/"/>
    
      <category term="特征交叉" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89/"/>
    
  </entry>
  
  <entry>
    <title>AUC 理解与应用</title>
    <link href="https://guyuecanhui.github.io/2020/04/04/auc/"/>
    <id>https://guyuecanhui.github.io/2020/04/04/auc/</id>
    <published>2020-04-04T13:54:19.000Z</published>
    <updated>2020-04-21T15:20:32.014Z</updated>
    
    <content type="html"><![CDATA[<p><strong>AUC</strong> 是一个对分类器预测数值不敏感的指标，具有比较好的稳定性，也因此成为推荐系统中最常用的模型离线指标之一。但是就这个如此常见的指标，很多人对它也只是一知半解，知道该用它，也知道调用函数去计算它，但是对它的计算过程、它还有哪些用处，却知之甚少。这篇文章就再唠叨一下 <strong>AUC</strong> 的概念、计算过程以及它如何应用于特征选择上。</p><h2 id="AUC-Area-Under-the-Curve-的概念"><a href="#AUC-Area-Under-the-Curve-的概念" class="headerlink" title="AUC (Area Under the Curve) 的概念"></a>AUC (Area Under the Curve) 的概念</h2><p><strong>AUC</strong>（Area Under Curve）被定义为 <strong>ROC</strong> 曲线下与坐标轴围成的面积。<strong>AUC</strong> 主要用来评估二分类问题的准确率，即随机给定一个正样本和一个负样本，分类器预测该正样本的得分比预测该负样本的得分大的概率。</p><p><strong>AUC</strong> 越接近 1 表明这个分类器的性能越好，随机猜测的 <strong>AUC</strong> 接近 0.5，如果一个分类器的 <strong>AUC</strong> 低于 0.5，只需要将它的结果取反就能一下子提高模型性能。</p><h2 id="AUC-的计算"><a href="#AUC-的计算" class="headerlink" title="AUC 的计算"></a>AUC 的计算</h2><p>根据 <strong>AUC</strong> 的含义，给定样本的真实标签和预测得分，计算分类器的 <strong>AUC</strong> 指标，主要有三种方法：梯形法、穷举法和序数法。</p><h3 id="梯形法-trapezoid-method"><a href="#梯形法-trapezoid-method" class="headerlink" title="梯形法 (trapezoid method)"></a>梯形法 (trapezoid method)</h3><p>梯形法就是先根据真实标签和预测得分，画出 <strong>ROC</strong> 曲线，然后简单地将每个相邻的点以直线连接，计算连线下方的总面积。<strong>ROC</strong> 曲线的详细说明可以参考 wiki，这里给出一个简单的绘制方法：</p><ol><li>计算所有样本中正样本数 <script type="math/tex">n_{pos}</script> 和负样本数 <script type="math/tex">n_{neg}</script>，进一步计算 x 轴的步长为 <script type="math/tex">s_x=\frac{1}{n_{neg}}</script>，y 轴的步长为 <script type="math/tex">s_y=\frac{1}{n_{pos}}</script>；</li><li>初始化 <strong>ROC</strong> 曲线在原点 $(0,0)$；</li><li>将所有样本按预测得分降序排列，然后按顺序进行以下判断并绘制：<ul><li>如果当前样本为正样本，则 <strong>ROC</strong> 曲线沿 y 轴向上移动单位步长 $s_y$；</li><li>如果当前样本为负样本，则 <strong>ROC</strong> 曲线沿 x 轴向右移动单位步长 $s_x$；</li><li>假设有连续 $k$ 个样本预测得分相同，其中有 $k_p$ 个正样本和 $k_n$ 个负样本，则 <strong>ROC</strong> 曲线沿着向量 $(k_n\cdot s_x,k_p\cdot s_y)$ 进行移动；</li></ul></li></ol><p>有了 <strong>ROC</strong> 曲线，我们可以用梯形法计算 <strong>ROC</strong> 曲线下的面积。举个例子：假设有 6 条样本，某个分类器对每个样本的预测得分 <script type="math/tex">\hat{y}</script> 及其真实标签 <script type="math/tex">y</script> 如下表所示：</p><div class="table-container"><table><thead><tr><th style="text-align:center">ID</th><th style="text-align:center">$\hat{y}$</th><th style="text-align:center">$y$</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">0.1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">0.3</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">0.5</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">0.5</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">0.7</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">0.9</td><td style="text-align:center">1</td></tr></tbody></table></div><p>则根据上面的 <strong>ROC</strong> 画法，我们计算出步长 $s_x=s_y=\frac{1}{3}$，再通过依次观察 F $\rightarrow$ A 的真实标签，可以画出如下的红色 <strong>ROC</strong> 曲线，并计算出曲线下的面积为：$\frac{1}{3}\times \frac{1}{3}+\frac{1}{2}\times(\frac{1}{3}+\frac{2}{3})\times\frac{1}{3}+\frac{1}{3}=0.61$。</p><img src="/2020/04/04/auc/roc.png" title="图 1. ROC 曲线示例"><h3 id="穷举法"><a href="#穷举法" class="headerlink" title="穷举法"></a>穷举法</h3><p>穷举法就是穷举样本中所有的正负样本对并计数，从而计算这些样本对中正样本预测得分大于负样本预测得分的比例。假设正例集合为 <script type="math/tex">N_{pos}</script>，正例数量为 <script type="math/tex">n_{pos}</script> ；负例集合为 <script type="math/tex">N_{neg}</script>，负例数量为 <script type="math/tex">n_{neg}</script>，则：</p><script type="math/tex; mode=display">AUC = \frac{\sum I(\hat{y}_{pos}, \hat{y}_{neg})}{n_{pos}\cdot n_{neg}}\qquad(1)</script><p>其中，<script type="math/tex">I(.,.)</script> 是指示函数：</p><script type="math/tex; mode=display">I(\hat{y}_{pos}, \hat{y}_{neg})=\begin{cases}1, \qquad &\hat{y}_{pos} > \hat{y}_{neg} \\0.5, \qquad &\hat{y}_{pos} = \hat{y}_{neg} \\0, \qquad &\hat{y}_{pos} < \hat{y}_{neg}\end{cases} \qquad(2)</script><p>例如，在上一小节中的例子中有 3 个正例 (BDF) 和 3 个负例 (ACE)，一共有 $3\times 3=9$ 种组合，每种组合的得分如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">负正样本对</th><th style="text-align:center">指示函数得分</th></tr></thead><tbody><tr><td style="text-align:center">AB</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">AD</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">AF</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">CB</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">CD</td><td style="text-align:center">0.5</td></tr><tr><td style="text-align:center">CF</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">EB</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">ED</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">EF</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">合计</td><td style="text-align:center">5.5</td></tr></tbody></table></div><p>因此，该分类器的 $AUC=\frac{5.5}{9}=0.61$，是个相当差的分类器了。</p><h3 id="序数法"><a href="#序数法" class="headerlink" title="序数法"></a>序数法</h3><p>穷举法比较适合小数据量情况下的计算，但是其实其中有很多比较是重复的。例如当我们按上例中 $\hat{y}$ 将样本排好序以后，对某个正样本而言，它上面的所有负样本数就是它的得分，没必要再穷举其他的样本对。因此我们考虑将样本排序后，基于样本序数来计算 <strong>AUC</strong>，这里略去推导过程，直接给出计算方法：</p><script type="math/tex; mode=display">AUC=\frac{\sum_{i\in N_{pos}}r_i - \frac{n_{pos}(1+n_{pos})}{2}}{n_{pos}\cdot n_{neg}}\qquad(3)</script><p>其中，$r_i$ 就是按 $\hat{y}$ 排序后，第 $i$ 个样本的序号，如果有多个样本预测得分相同，则这些样本的序号直接求平均。例如对于上面的例子，我们可以对这些样本做如下的编号：</p><div class="table-container"><table><thead><tr><th style="text-align:center">ID</th><th style="text-align:center">$\hat{y}$</th><th style="text-align:center">$y$</th><th style="text-align:center">$r$</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">0.1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">0.3</td><td style="text-align:center">1</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">0.5</td><td style="text-align:center">0</td><td style="text-align:center">3.5</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">0.5</td><td style="text-align:center">1</td><td style="text-align:center">3.5</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">0.7</td><td style="text-align:center">0</td><td style="text-align:center">5</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">0.9</td><td style="text-align:center">1</td><td style="text-align:center">6</td></tr></tbody></table></div><p>其中，CD 的预测得分相同，因此它们的编号为 $\frac{3+4}{2}=3.5$。根据式 $(3)$ 我们可以算出所有正样本的序数和为 $2+3.5+6=11.5$，$AUC=\frac{11.5-3\times 4\div 2}{3\times 3}=0.61$。</p><h3 id="Group-AUC"><a href="#Group-AUC" class="headerlink" title="Group AUC"></a>Group AUC</h3><p>上面介绍了通常意义下 <strong>AUC</strong> 的计算方法，这个指标实际上是将所有的正负样本放在一起，来衡量模型整体的排序能力。但是在推荐场景下，实际上我们更关心的是<strong>对单个用户而言，推荐结果的相对顺序</strong>，至于不同用户之间是否正样本的得分一定比负样本高，其实影响并不大。</p><p>因此，在某些场景下 <strong>AUC</strong> 可能无法真实的度量模型的排序能力 (比如离线指标相对较高，但是线上指标相对较差)，这时可以考虑尝试使用 <strong>GAUC</strong> 来度量。</p><p><strong>GAUC</strong> 实际上就是按用户进行分组，计算每个用户下样本排序的 <strong>AUC</strong>，再汇总加权得分，计算公式如下：</p><script type="math/tex; mode=display">GAUC=\frac{\sum_{u}w_u \cdot AUC_u}{\sum_{u}w_u}\qquad(4)</script><p>这里的用户权重 <script type="math/tex">w_u</script> 可以设置为该用户的点击数/曝光数。至于每个用户样本的 <script type="math/tex">AUC_u</script> 计算还是参考式 $(1)$ 或式 $(3)$。</p><h2 id="单特征-AUC-的计算"><a href="#单特征-AUC-的计算" class="headerlink" title="单特征 AUC 的计算"></a>单特征 AUC 的计算</h2><p>特征选择中一个重要的技术就是单特征 <strong>AUC</strong> 的计算，它体现了这维特征正确划分样本的能力。单特征 <strong>AUC</strong> 越高，这维特征也就越重要，尤其是在线性模型中。</p><h3 id="单值离散特征的-AUC-计算"><a href="#单值离散特征的-AUC-计算" class="headerlink" title="单值离散特征的 AUC 计算"></a>单值离散特征的 AUC 计算</h3><p>单值离散特征是指特征取值是离散特征，而且每个样本中该特征取值只有一个，例如用户的城市、性别等。我们基于训练数据和测试数据可以使用以下算法高效的计算该特征的 <strong>AUC</strong> (推导过程参考<a href="https://blog.csdn.net/u013019431/article/details/92851053" target="_blank" rel="noopener">这里</a>)：</p><ol><li>计算这维特征的每个特征值在训练数据中的正样本占比；</li><li>在测试数据中，用第 1 步计算的结果作为样本的预测值；</li><li>基于上一小节中的 <strong>AUC</strong> 计算方法计算理论上该特征在 <strong>LR</strong> 下的 <strong>AUC</strong>；</li></ol><p>举个例子，我们考虑性别特征的单值 <strong>AUC</strong> 计算，假设有 10 条样本 (前 6 条作为训练样本，后 4 条作为测试样本)：</p><div class="table-container"><table><thead><tr><th style="text-align:center">ID</th><th style="text-align:center">gender</th><th style="text-align:center">$y$</th><th style="text-align:center">$\hat{y}$</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">m</td><td style="text-align:center">0</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">f</td><td style="text-align:center">1</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">m</td><td style="text-align:center">1</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">f</td><td style="text-align:center">0</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">f</td><td style="text-align:center">1</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">m</td><td style="text-align:center">0</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">G</td><td style="text-align:center">m</td><td style="text-align:center">1</td><td style="text-align:center">0.33</td></tr><tr><td style="text-align:center">H</td><td style="text-align:center">f</td><td style="text-align:center">1</td><td style="text-align:center">0.67</td></tr><tr><td style="text-align:center">I</td><td style="text-align:center">f</td><td style="text-align:center">1</td><td style="text-align:center">0.67</td></tr><tr><td style="text-align:center">J</td><td style="text-align:center">m</td><td style="text-align:center">0</td><td style="text-align:center">0.33</td></tr></tbody></table></div><p>我们先根据训练样本 A-F 计算性别为 m 的正样本比例为 $0.33$，性别为 f 的正样本比例为 $0.67$，因此我们预测样本 G-J 的得分依次为 $[0.33,0.67,0.67,0.33]$，从而根据式 $(1)$ 可以很快算出来，这维特征的 <strong>AUC</strong> 为 $(0.5+1+1)/3=0.83$。</p><h3 id="多值离散特征的-AUC-评估"><a href="#多值离散特征的-AUC-评估" class="headerlink" title="多值离散特征的 AUC 评估"></a>多值离散特征的 AUC 评估</h3><p>多值离散特征是指特征取值是离散特征，而且每个样本中该特征取值可能有多个，例如视频的标签、演员等。虽然单值离散特征可以高效的计算它的 <strong>AUC</strong>，遗憾的是，多值离散特征的 <strong>AUC</strong> 似乎无法进行类似的推导。因此我们目前还是通过仅使用这一维特征构建训练数据，用于训练 <strong>LR</strong> 模型，再用该 <strong>LR</strong> 模型预测测试数据的得分，用模型的 <strong>AUC</strong> 来作为该特征的 <strong>AUC</strong>。</p><p>实际上，单值离散特征也可以用建模型的方式来评估，我们两种方法都尝试了，结果证明两种方法得到的结果是十分接近的。</p><h3 id="连续特征的-AUC-评估"><a href="#连续特征的-AUC-评估" class="headerlink" title="连续特征的 AUC 评估"></a>连续特征的 AUC 评估</h3><p>连续特征就是指取值是连续值的特征，比如视频的播放次数、播放完成度等。</p><p>有一些特征如年龄，虽然是连续值，但是因为取值有限，既可以作为连续特征，也可以作为离散特征。但是连续特征，尤其是非均匀分布、或者偏序关系不明确的连续特征，对于线性模型是不太友好的，例如我们考虑预测用户是否播放一个恐怖电影，可能年龄大的或者年龄小的用户都不喜欢看，而比较年轻的用户最喜欢，线性模型就难以区分了。</p><p>因此，如果在线性模型中使用连续值特征，除了像播放完成度这类取值范围有限且偏序关系一致 (播放完成度越高，通常表明视频质量越好，用户点击的概率也越大) 的连续特征，我们最好还是按下面的算法来评估某连续特征的单特征 <strong>AUC</strong>：</p><ol><li>在训练数据中使用该连续特征训练 <strong>GBDT</strong> 模型；</li><li>如果第 1 步中使用多棵树，则用第 1 步训练的 <strong>GBDT</strong> 对测试数据中的该连续特征进行分桶，将分桶结果作为新的多值离散特征，并使用多值离散特征的 <strong>AUC</strong> 评估方法进行评估；</li><li>如果第 1 步中使用单棵树，则用第 1 步训练的 <strong>GBDT</strong> 对测试数据中的该连续特征进行分桶，将分桶结果作为新的单值离散特征，并使用单值离散特征的 <strong>AUC</strong> 计算方法进行评估；</li></ol><p>至于使用单棵树还是多棵树，最好参数与模型实际执行的特征变换保持一致。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;AUC&lt;/strong&gt; 是一个对分类器预测数值不敏感的指标，具有比较好的稳定性，也因此成为推荐系统中最常用的模型离线指标之一。但是就这个如此常见的指标，很多人对它也只是一知半解，知道该用它，也知道调用函数去计算它，但是对它的计算过程、它还有哪些用处，却知之
      
    
    </summary>
    
      <category term="特征工程" scheme="https://guyuecanhui.github.io/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="指标" scheme="https://guyuecanhui.github.io/tags/%E6%8C%87%E6%A0%87/"/>
    
      <category term="AUC" scheme="https://guyuecanhui.github.io/tags/AUC/"/>
    
  </entry>
  
  <entry>
    <title>DICM with AMS 论文精读</title>
    <link href="https://guyuecanhui.github.io/2019/11/24/paper-2018-ali-dicm/"/>
    <id>https://guyuecanhui.github.io/2019/11/24/paper-2018-ali-dicm/</id>
    <published>2019-11-24T13:50:20.000Z</published>
    <updated>2019-12-08T15:28:38.790Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文引用：Ge, Tiezheng , et al. “Image Matters: Visually modeling user behaviors using Advanced Model Server.” (2018).</p></blockquote><p>本文是阿里发表在 <strong>CIKM 2018</strong> 上的文章，主要思路是将用户历史有过行为 (文章实际使用了点击行为) 的图片来对用户视觉兴趣进行建模，在广告 <strong>CTR</strong> 预估时就能够估计用户对广告图片的喜好，从而提升 <strong>CTR</strong> 预估的准确率。由于用户在商品页面行为历史通常较为丰富，因此训练样本中会包含大量的图片特征，这些特征使用传统的 <strong>PS</strong> 架构无法有效训练，因此文章提出了 <strong>AMS</strong> (<em>Advanced Model Server</em>) 架构，能够平衡存储和通信开销，使得天级更新模型成为可能。这篇文章主要的贡献也是在工程实现方面。</p><h3 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a>背景和动机</h3><p>广告 <strong>CTR</strong> 预估是广告系统中至关重要的环节，传统的 (非多模态) 预估模型，主要是使用一些 <strong>ID</strong> 和交叉统计之类的特征，但是由于 <strong>ID</strong> 没有语义，我们无法判断两个 <strong>ID</strong> 是不是有相关性，因此这些模型对于出现频次少或者新出现的 <strong>ID</strong> 无法充分训练，即存在冷启动的问题。</p><p>解决冷启动问题通常会想到引入内容特征，而在电商领域，图片就是能够描述广告/商品的最直观且最受用户关注的内容特征。用户在购买商品的时候，通常会点击、浏览商品的图片，结合商品的描述、评论等其他信息来决定是否购买，因此用户曾经点击过的图片能够在很大程度上表征用户的兴趣。而用户是否点击广告，图片、标题等信息是影响最大的因素。因此，文章就考虑使用用户点击过的图片来建立用户的兴趣模型，再与广告的图片进行匹配，从而估计用户在视觉层面是否对该广告有兴趣；将这方面的估计与传统的基于 <strong>ID</strong> 等特征的估计相结合，来提升 <strong>CTR</strong> 估计的性能。基于这个思路，文章提出了 <strong>DICM</strong> (<em>Deep Image CTR Model</em>)。 </p><p>使用用户图片历史来建立用户视觉偏好主要的问题就是数据传输和存储，如果使用传统的 <strong>PS</strong> 架构，图片数据是保存在 <strong>KV Store</strong> 里（也就是参数服务器），谁用谁来查一下，而图片的数量和数据量都是相对较大的，因此这种方式十分消耗资源，并且这种方式也难以针对 <strong>CTR</strong> 预估任务对预训练的图片特征进行进一步的组合和压缩，即图片数据不是目标相关的。文章针对这个问题，提出了一种扩展 <strong>PS</strong> 架构，称为 <strong>AMS</strong> 架构。下面就详细的介绍这两方面的工作。</p><h3 id="AMS-架构"><a href="#AMS-架构" class="headerlink" title="AMS 架构"></a>AMS 架构</h3><p>如上所述，文章在实现 <strong>DICM</strong> 模型时，实际上是基于 <strong>AMS</strong> 架构的。<strong>AMS</strong> 架构整体如图 1 右图所示：</p><img src="/2019/11/24/paper-2018-ali-dicm/ams-architecture.png" title="图 1. AMS 架构及基于 AMS 架构实现的 DICM 模型"><p><strong>AMS</strong> 将节点分为 <strong>Worker</strong> 和 <strong>Server</strong> 组：</p><ul><li><strong>Server 组</strong>：保存图片原始特征数据（考虑到端到端训练和推理的时延，这里取的是预训练模型的低阶隐层输出），并且负责将图片原始特征数据映射到任务空间的高阶表达，实际上就是学一个 Tower 模型，这个 Tower 模型是所有 Server 共享的。使用这个图片 <strong>EmbedTower</strong>，可以将图片原始特征数据极大的压缩（文章使用 $4096\times 256\times 64\times 12$ 的 Tower，可以将数据量减少 340 多倍），Worker 查询的时候传输数据量就大大减少了。</li><li><strong>Worker 组</strong>：从 <strong>Server 组</strong>查询样本的各维特征（包括 <strong>ID</strong> embedding 和图片的高阶 embedding 等），将特征组合后进行推理；训练时还要将损失梯度传回 Server，用来更新图片 <strong>EmbedTower</strong>。</li></ul><p>这样分与传统 <strong>PS</strong> 架构最大的差别就在于，<strong>Server 组</strong>内的节点也是有通信的，也是需要更新模型的了 (更详细的比较可以参考文献 [1]，说的很清楚)。好处是，当图片的数据量非常大的时候：</p><ol><li>每个图片原始特征只保存在单个 Server 上，节约了存储；</li><li>每个图片的 embedding 只需要计算一次，而且可以通过预计算，缓存以后供多个 Worker 查询；</li><li>压缩后的图片 embedding 数据量小，减小了数据传输消耗；</li></ol><p>文章号称在他们的场景下能节省 31 倍的存储开销和 340 倍的传输开销，而推理的时延仅仅增加了 3 毫秒。不过大家在各自具体场景中可能需要权衡一下性价比。</p><h3 id="DICM-模型"><a href="#DICM-模型" class="headerlink" title="DICM 模型"></a>DICM 模型</h3><p>从图 1 左图来看，<strong>DICM</strong> 模型整体就是一个简单的 Embedding + MLP 架构。其中，最关键的部分实际上就是用户视觉偏好抽取的部分（后面简称为 <strong>VisualPrefExtractor</strong>），也就是图片 <strong>EmbedTower</strong> 和基于用户图片历史和当前广告图片生成兴趣特征向量的部分。</p><p><strong>EmbedTower</strong> 前面简单介绍过了，这里需要说明的是，文章在保存图片原始特征的时候，并不是使用原始的像素值，而是经过预训练的 <strong>VGG16</strong> 第 14 层的 <strong>FC-4096</strong> 作为原始特征（如图 2 所示）。虽然 <strong>VGG16</strong> 的训练目标是图像分类的任务，但是这种任务学到的语义特征有比较好的泛化性能，而且由于是逐层处理，取靠前的参数实际上使用的是图像的一些基础元素的特征，这些特征再进一步通过广告 <strong>CTR</strong> 预估任务进行训练，这样就能够得到有用的高阶特征。文章也尝试了使用其他层的输出作为图片的原始特征：太靠近输出会损失性能；而越靠近输入端，参数越多，并且边际效益递减，因此这也只是一种权衡。另外，淘宝主要是买商品，使用 <strong>ImageNet</strong> 的预训练模型可能比较契合，我们在视频场景下就会发现，<strong>VGG16</strong> 对于人物的识别权重很低，因此可能又不太适用。</p><img src="/2019/11/24/paper-2018-ali-dicm/vgg16.png" title="图 2. 使用配置 D (VGG16) 中第 1 个 FC-4096 作为图片的原始特征，该配置参考文献 [2]"><p>而为了抽取用户的视觉偏好，我们需要从用户图片历史序列中抽取出有效的特征。在用户行为非常丰富的场景，比如淘宝，用户的兴趣是多元化的，用户是不是点击某件 T-恤的广告，主要取决他历史上对衣服款式的喜好，而受他买零食、饮料、矿泉水的影响较小，因此需要引入注意力机制来针对不同的广告从用户图片历史中抽取出不同的特征表达。</p><img src="/2019/11/24/paper-2018-ali-dicm/attentive-pooling.png" title="图 3. 几种 Pooling 方式的比较，文章使用 (d) 所示的 AttentivePooling"><p>文章使用的注意力机制还是比较简单的，如图 3 右图所示，就是将广告图片特征和用户每条历史图片特征拼接后，经过一个 $64\times 16\times 1$ 的 Tower，用来计算每条历史图片的权重，然后再加权对所有历史图片特征计算 sum pooling，得到用户视觉偏好表达。这里的广告图片特征和用户历史图片特征都是经过 <strong>EmbedTower</strong> 映射后的高阶表达。为了增强记忆能力，文章还使用了 <strong>ID</strong> 行为列表来做相似的处理，将得到的 embedding 与图像偏好的 embedding 拼接，即实现了 <strong>MultiQueryAttentivePooling</strong>，两者互为补充，效果得到进一步提升。</p><p>这种方式很容易扩展，比如可以简单的添加 <strong>TextPrefExtractor</strong>、<strong>AudioPrefExtractor</strong> 等其他多模态的偏好模型，而且由于这些多模态数据不与其他特征进行交叉，因此训练与推理都是相对独立的。<strong>VisualPrefExtractor</strong> 也可以作为子模型嵌入到双塔结构里做召回，或者嵌入其他复杂排序模型，如 <strong>DIN</strong> 等，作为特征抽取器。</p><p>根据这篇文章的说明，该模型至少在当时是承接了淘宝的主流量，因此在多模态方面还是十分值得借鉴的。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] 一图胜千言: 解读阿里的Deep Image CTR Model. <a href="https://zhuanlan.zhihu.com/p/57056588" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/57056588</a>.</p><p>[2] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;论文引用：Ge, Tiezheng , et al. “Image Matters: Visually modeling user behaviors using Advanced Model Server.” (2018).&lt;/p&gt;
&lt;/bloc
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="多模态" scheme="https://guyuecanhui.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
      <category term="预排序" scheme="https://guyuecanhui.github.io/tags/%E9%A2%84%E6%8E%92%E5%BA%8F/"/>
    
      <category term="图片" scheme="https://guyuecanhui.github.io/tags/%E5%9B%BE%E7%89%87/"/>
    
      <category term="分布式" scheme="https://guyuecanhui.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="注意力" scheme="https://guyuecanhui.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
  </entry>
  
  <entry>
    <title>MMoE-PosBias 论文精读</title>
    <link href="https://guyuecanhui.github.io/2019/11/16/paper-2019-google-mmoe-bias/"/>
    <id>https://guyuecanhui.github.io/2019/11/16/paper-2019-google-mmoe-bias/</id>
    <published>2019-11-16T14:19:16.000Z</published>
    <updated>2019-11-17T02:25:29.104Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Zhao, Zhe, et al. “Recommending what video to watch next: a multitask ranking system.” <em>Proceedings of the 13th ACM Conference on Recommender Systems</em>. ACM, 2019. </p></blockquote><p>本文是 Youtube 发表在 <strong>RecSys</strong> <strong>2019</strong> 上的文章，主要解决的问题是提升用户的总体满意度，同时减少推荐造成的用户选择偏差对推荐系统的影响。解决这些问题主要的挑战在于：</p><ol><li><strong>视频推荐中包含多个可能互相冲突的目标，难以权衡</strong>。视频推荐的目标大体可以分为 <strong>engagement objectives</strong> (例如点击、播放等)、<strong>satistaction objective</strong> (例如点赞、收藏、不喜欢等)，这两类目标可能有冲突，例如用户点赞的视频可能是他比较喜欢的严肃的视频，但是用户真正播放的视频可能是一些娱乐性比较强的。</li><li><strong>推荐系统会引入一些隐式的偏差，尤其是推荐位置导致的用户选择偏差</strong>。用户往往倾向于点位置靠前的视频，而这些视频可能并不是用户真实喜欢的视频。如果推荐模型使用带偏差的用户行为日志进行训练，会进一步强化这种偏差，导致恶性循环。</li><li><strong>多模态特征</strong>。多模态特征包括语音、视频 、图像、文本、ID 类特征、连续型特征等。使用多模态的特征有助于信息互补，例如我们希望对低层次内容特征进行映射来跨越语义鸿沟、用于基于内容的过滤；同时将稀疏分布的 ID 类特征用于协同过滤等。</li><li><strong>可扩展性</strong>。模型需要考虑线下训练和线上服务的性能，因此在保证学习效果的情况下，尽可能使用简单易扩展的网络架构。</li></ol><p>为了应对这些挑战，文章提出了如下图所示的排序网络架构。该网络接收用户当前播放的视频信息和上下文信息，对数百个召回视频进行排序。这里的召回需要是从不同的角度进行召回，例如基于内容相似的召回、基于协同过滤的召回等，文章使用了 <strong>Deep Candidate Generation</strong> 模型$^{[1]}$等来生成召回。</p><img src="/2019/11/16/paper-2019-google-mmoe-bias/overview.png" title="MMoE with PosBias 整体架构"><p>排序的整体架构服从 <strong>Wide &amp; Deep</strong> 架构，如上图所示。其中，左边的 <strong>wide</strong> 层是一个浅层的 <strong>tower</strong> 网络，用于学习选择偏差，这个偏差在上层会与 <strong>user engagement</strong> 的输出相加，用于抵消选择偏差；右边的 <strong>deep</strong> 层是一个 <strong>MMoE</strong> 的多目标网络（<strong>MMoE</strong> 的解读可以参考 [2]），用于从不同模态特征中学习不同的专家网络，并学习多个 <strong>Gate</strong> 来对多个目标进行预测，最后再对多个目标进行权衡，这里不同的目标使用不同的损失函数，对于目标是连续值的，使用 <strong>MSE</strong> 进行回归，对于目标是离散值的，使用 <strong>logLoss</strong> 进行分类，最终多个目标的结果使用线性加权的方式计算出最终的得分，权重目标是作者手调的。从训练和预测的工程效率角度出发，文章选择了最简单的 <strong>point-wise ranking</strong> 方案。</p><img src="/2019/11/16/paper-2019-google-mmoe-bias/mmoe.png" title="MMoE 架构拆解"><p>多目标的深度网络是整体架构的主要核心，文章先将所有原始的输入压缩成了一个共享的隐含层，再基于该隐含层构建了 <strong>MMoE</strong> 的网络，基于这些网络再进一步学习多个目标。其中：</p><ol><li><p>隐含层的设置主要是为了提高训练的效率。直接用原始特征来训练 <strong>MMoE</strong> 网络可能对多模态特征的学习更有利，但是由于原始特征的维度过高，直接基于原始特征进行训练代价过高。在学习 <strong>Gate</strong> 的时候，作者也尝试过直接用原始特征作为输入，但是比用共享隐含层作为输入没有显著提升。</p></li><li><p>使用 <strong>MoE</strong> 层比使用 <strong>shared-bottom</strong> 策略更有助于学习多模态特征，文章使用了少量的 <strong>Expert</strong> 网络，这样可以在不同的 <strong>Gate</strong> 中充分共享 <strong>Expert</strong> 网络参数，并且工程上效率更高。</p></li><li><p>每个目标使用一个 <strong>Gate</strong> 来对原始特征和专家网络进行激活，每个 task 预测结果的变换函数如下：</p><script type="math/tex; mode=display">\begin{cases}y_k=h^k(f^k(x)) \\f^k(x)=\sum_{i=1}^n g_i^k(x)\cdot f_i(x) \\g^k(x) = \text{softmax}(W_{g^k}\cdot x)\end{cases}</script><p>其中，<script type="math/tex">x</script> 为共享隐含层，<script type="math/tex">f_i(x)</script> 为第 $i$ 个专家网络的输出，$n$ 为专家网络的个数，<script type="math/tex">W_{g^k}</script> 为第 $k$ 个 <strong>Gate</strong> 的网络参数，<script type="math/tex">h^k(x)</script> 为第 $k$ 个目标的隐含层。</p></li></ol><img src="/2019/11/16/paper-2019-google-mmoe-bias/bias.png" title="PosBias 架构拆解"><p>为了减少由于推荐产生的用户选择偏差，文章又增加了一个浅层网络，如上图所示，输入是与偏差相关的特征（简称偏差特征），主要包括物品展示的位置特征和用户的设备信息。学习的时候，为了减少过位置的过分依赖，对所有偏差特征扫行 10% 的 <strong>dropout</strong>。</p><p>学习选择偏差的其他策略包括：直接把偏差特征作为输入的一部分进行训练，这种方式主要用于线性网络，在深度网络里效果不好；使用 <strong>Adversarial Learning</strong>，即将位置作为一个辅助的学习目标来预测。文章的实验结果也表明还是使用浅层网络的效果最好，直接将偏差特征作为输入的效果还不如不加偏差特征。</p><p>以上关于模型的部分其实简洁明了，没有太多的花招。但是特征层就难以企及了，而且在工程效率上必然也是困难重重，文章中工程的部分以后在应用的时候可能还需要再细细品读。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li>Covington, Paul , J. Adams , and E. Sargin . “<strong>Deep Neural Networks for YouTube Recommendations.</strong>“ <em>Acm Conference on Recommender Systems</em> ACM, 2016:191-198.</li><li><strong>MMoE 论文精读</strong>. <a href="https://guyuecanhui.github.io/2019/11/16/paper-2018-google-mmoe/">https://guyuecanhui.github.io/2019/11/16/paper-2018-google-mmoe/</a>.</li><li><strong>YouTube 多目标排序系统：如何推荐接下来收看的视频</strong>. <a href="https://www.infoq.cn/article/cZNdKS5ssPiqhjCyKQZ6" target="_blank" rel="noopener">https://www.infoq.cn/article/cZNdKS5ssPiqhjCyKQZ6</a>.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Zhao, Zhe, et al. “Recommending what video to watch next: a multitask ranking system.” &lt;em&gt;Proceedings of the 13th ACM Confe
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="多任务" scheme="https://guyuecanhui.github.io/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"/>
    
      <category term="MoE" scheme="https://guyuecanhui.github.io/tags/MoE/"/>
    
      <category term="选择偏差" scheme="https://guyuecanhui.github.io/tags/%E9%80%89%E6%8B%A9%E5%81%8F%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>MMoE 论文精读</title>
    <link href="https://guyuecanhui.github.io/2019/11/16/paper-2018-google-mmoe/"/>
    <id>https://guyuecanhui.github.io/2019/11/16/paper-2018-google-mmoe/</id>
    <published>2019-11-16T01:42:59.000Z</published>
    <updated>2019-11-17T02:19:59.869Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文引用: Ma, Jiaqi , et al. “Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts.” <em>the 24th ACM SIGKDD International Conference</em> ACM, 2018.</p></blockquote><p>本文是 Google 发表在 <strong>KDD 2018</strong> 的论文，不过感觉少了一些工程的加持，内容略显单薄。文章主要提出了一种多专家子网的结构，显式的从数据中学习多个任务之间的关系，并能够通过门限网络对每个任务进行单独的优化。与传统的 <strong>share-bottom</strong> 结构相比，这种结构在任务之间关联较弱时，仍然能够取得比较好的效果。</p><p>近年来，在推荐领域逐渐引入多任务学习来减轻一些使用单个模型指标可能带来的负面影响。例如在视频推荐中，只考虑点击转化率时，会倾向推荐包含标题党、擦边海报的视频；只考虑完成度时，会倾向推荐时间比较短的视频等等。而这些倾向都会影响用户体验，并且可能导致业务长期目标的下降。因此，大家开始尝试引入多个相互关联但又不一致的目标来进行综合考虑建模，并且实践表示，多任务学习在推荐系统中能够提升上下文推荐的效果。</p><p>传统的基于神经网络的多任务学习大致分为两类，一共是底层参数共享，即共享输入到中间层的参数，上层再分别对各个任务建模；一类是参数软共享，并不显式的共享底层参数，而是通过正则等对多个任务的参数进行相互约束。目前看到的比较多的是第一种，如下图 (a) 所示。而这种方式一般都假设多个任务的数据分布和目标都是相似的，当任务间差异变大时，对某些任务的预测性能就会产生较大的影响。然而实际任务的相关性都是难以度量的，因此效果实际上无法事先评估，只能靠不断尝试。</p><img src="/2019/11/16/paper-2018-google-mmoe/mmoe.png" title="MMOE 架构比较"><p>本文的作者受到 <strong>MoE</strong> 网络$^{[1]}$的启发，在多任务学习中引入 <strong>MoE</strong> 层，来显式的对多个任务的关系进行建模，或者理解成学习所有任务的不同方面；再对每个任务学习一个门限网络，这个门限网络可以理解成这个任务在各个方面的特点。整体结构如上图 (c) 所示。其中，每个共享的子网称为一个 <strong>Expert</strong>，文章中的 <strong>Expert</strong> 都使用前馈网络，它的输入是原始特征（也可以是一个共享的隐含层，直接使用原始特征效果会更好，但是维度可能过高），输出为各个 <strong>Gate</strong> 的权重分布（<strong>softmax</strong>），可以理解成是这个 <strong>Expert</strong> 对不同任务的影响程度。研究已经表明在 <strong>DNN</strong> 中，使用这种集成模型和集成子网络的方式有助于提高模型的性能。</p><p>文章在公开数据集和 Google 数据上进行了大量的对比实验，结果表明：</p><ol><li><strong>MMoE</strong> 在任务相关性变弱的情况下，性能影响较小，因此实用性也更强；</li><li><strong>MMoE</strong> 的训练误差收敛更快更稳定，即可训练性更好；这也与近年研究得出的结论一致，即 <em>Modulation and gating mechanisms can improve the trainability in training non-convex deep nurual networks</em>。</li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li>Jacobs, Robert A. , et al. “<strong>Adaptive Mixtures of Local Experts.</strong>“ <em>Neural Computation</em> 3.1(1991):79-87.</li><li><strong>keras-mmoe</strong>: <a href="https://github.com/drawbridge/keras-mmoe" target="_blank" rel="noopener">https://github.com/drawbridge/keras-mmoe</a>.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;论文引用: Ma, Jiaqi , et al. “Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts.” &lt;em&gt;the 24
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="多任务" scheme="https://guyuecanhui.github.io/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"/>
    
      <category term="MoE" scheme="https://guyuecanhui.github.io/tags/MoE/"/>
    
  </entry>
  
  <entry>
    <title>ESMM 论文精读</title>
    <link href="https://guyuecanhui.github.io/2019/11/09/paper-2018-ali-esmm/"/>
    <id>https://guyuecanhui.github.io/2019/11/09/paper-2018-ali-esmm/</id>
    <published>2019-11-09T12:16:34.000Z</published>
    <updated>2019-11-16T01:47:27.694Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文引用: Ma, Xiao, et al. “Entire space multi-task model: An effective approach for estimating post-click conversion rate.” The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. ACM, 2018.</p></blockquote><p>本文是阿里发表在 <strong>SIGIR 2018</strong> 年的短文，主要解决了精确预估 <strong>CVR</strong> 的问题。<strong>CVR</strong> 预估是最大化场景商品交易总额 (<strong>GMV</strong>=<code>流量×点击率×转化率×客单价</code>) 的重要因子，它可以用于 <strong>OCPC</strong> 模式下动态调整出价来使平台和广告主共同受益；并且从用户体验的角度来说，准确预估的 <strong>CVR</strong> 被用来平衡用户的点击偏好与购买偏好。文章认为当前的 <strong>CVR</strong> 预估主要存在两个问题：</p><ol><li><strong>Sample Selection Bias (SSB)</strong>：当前 <strong>CVR</strong> 预估是基于 <code>点击-&gt;转化</code> 数据进行训练的，而有点击的展示数据只是所有展示数据中的一小部分 (如下图所示)，这部分数据的分布与整体的分布通常并不一致。而在实际 serving 的时候，模型又是对整个空间中的所有样本进行预测，因此模型的泛化效果会受到影响。</li><li><strong>Data Sparsity (DS)</strong>：与前一个问题的根因相同，只使用点击数据会存在严重的数据稀疏问题。</li></ol><img src="/2019/11/09/paper-2018-ali-esmm/esmm-ssb.png" title="用户展示-点击-转化行为关系示意"><p>业界也提出过一些解决这两个问题的方案：</p><ol><li><strong>SSB Solution</strong>：<strong>AMAN 方法</strong> 将所有展示未点击的数据也作为负样本进行训练，但是这种方法天然会导致 CVR 被低估 (因为对于一些展示未点击的物品，可能是因为用户并没有关注到，或者用户已经点击了其他的条目而遗漏，并非是真正不会产生转化的物品)；<strong>无偏估计方法</strong> 通过拒绝采样的方法来保证预估的 CVR 与真实的观察一致，但是这种方法在计算过程中会除以一个很小的数，因此可能导致数值不稳定的问题。</li><li><strong>DS Solution</strong>：<strong>分层建模方法</strong>使用不同的特征构建多个预估模型，然后使用 <strong>LR</strong> 等模型将这些模型的结果汇总，这种方法需要比较可靠的先验知识来构建分层模型，在数据量大的推荐场景下难以实现；<strong>过采样方法</strong>将数据量少的类别样本进行过采样，但是对采样参数十分敏感。</li></ol><p>文章在已有工作的基础上，提出使用多任务学习的框架，使用所有 <code>展示-&gt;点击-&gt;转化</code> 数据进行训练，将 <strong>CVR</strong> 预测问题转变为同时预测 <strong>CTR</strong> 和 <strong>CTCVR</strong> 的问题。由于使用所有展示样本，因此不存在 <strong>SSB</strong> 问题；在多任务学习下共享 embedding 向量，实际上是一种参数迁移学习，可以有效的解决 <strong>DS</strong> 问题。</p><p>具体来讲，将一个样本记为 $(\boldsymbol{x},y\rightarrow z)$，其中，$\boldsymbol{x}$ 表示样本特征，$y$ 表示是否点击，$z$ 表示是否转化。则：</p><script type="math/tex; mode=display">\begin{cases}pCTCVR = p(z=1,y=1|\boldsymbol{x}) = pCTR\times pCVR \\pCTR = p(y=1|\boldsymbol{x})\\pCVR = p(z=1|\boldsymbol{x},y=1)\end{cases}</script><p>由于这三个变量的自由度为 2，因此损失函数只需要计算其中两个即可。文章将损失函数设计为 <strong>CTR</strong> 和 <strong>CTCVR</strong> 的预测损失，如下所示：</p><script type="math/tex; mode=display">L(\theta_{cvr},\theta_{ctr}) = \sum_{i=1}^N l(y_i, f(\boldsymbol{x}_i;\theta_{ctr})) + \sum_{i=1}^N l(y_i\&z_i, f(\boldsymbol{x}_i;\theta_{ctr})\times f(\boldsymbol{x}_i;\theta_{cvr}))</script><p>整体网络架构如下图所示：</p><img src="/2019/11/09/paper-2018-ali-esmm/esmm-architect.png" title="ESMM 网络整体架构"><p>可以看到，两个任务共享底层 embedding，同时通过顶层的 <strong>Dot</strong> 算子进行关联。文章没有将 <strong>pCVR</strong> 作为最终输出的结果，是因为 $pCVR = \frac{pCTCVR}{pCTR}$，如果将 <strong>pCVR</strong> 作为最终输出，则最后一步为除法算子，而除法具有数值不稳定性，可能会得出 $pCVR&gt;1$ 的情况，因此将 <strong>pCTCVR</strong> 作为最终输出的结果，这样能够保证 <strong>pCVR</strong> 的结果在 $[0,1]$ 范围内，避免了数值不稳定的问题。</p><p>文章在淘宝数据上与现有解决 <strong>SSB</strong> 和 <strong>DS</strong> 问题的几个策略进行了对比验证，发现基于 <strong>ESSM</strong> 模型的 <strong>CVR</strong> 和 <strong>CTCVR</strong> 预估任务的 <strong>AUC</strong> 是最高的。而且文章还发表了一个 mini 公开数据集，诚意满满~</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><strong>XDL ESSM</strong>: <a href="https://github.com/alibaba/x-deeplearning/tree/master/xdl-algorithm-solution/ESMM" target="_blank" rel="noopener">https://github.com/alibaba/x-deeplearning/tree/master/xdl-algorithm-solution/ESMM</a></li><li><strong>完整空间多任务模型：CVR预估的有效方法</strong>: <a href="http://xudongyang.coding.me/esmm/" target="_blank" rel="noopener">http://xudongyang.coding.me/esmm/</a></li><li><strong>构建分布式Tensorflow模型系列之CVR预估案例ESMM模型</strong>: <a href="http://xudongyang.coding.me/esmm-1/" target="_blank" rel="noopener">http://xudongyang.coding.me/esmm-1/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;论文引用: Ma, Xiao, et al. “Entire space multi-task model: An effective approach for estimating post-click conversion rate.” The
      
    
    </summary>
    
      <category term="论文精读" scheme="https://guyuecanhui.github.io/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="排序" scheme="https://guyuecanhui.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="多任务" scheme="https://guyuecanhui.github.io/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/"/>
    
      <category term="样本偏差" scheme="https://guyuecanhui.github.io/tags/%E6%A0%B7%E6%9C%AC%E5%81%8F%E5%B7%AE/"/>
    
      <category term="数据稀疏" scheme="https://guyuecanhui.github.io/tags/%E6%95%B0%E6%8D%AE%E7%A8%80%E7%96%8F/"/>
    
  </entry>
  
  <entry>
    <title>效率提升 10 倍的各种配置</title>
    <link href="https://guyuecanhui.github.io/2019/08/28/jupyter-config/"/>
    <id>https://guyuecanhui.github.io/2019/08/28/jupyter-config/</id>
    <published>2019-08-28T14:36:04.000Z</published>
    <updated>2020-04-08T15:00:19.310Z</updated>
    
    <content type="html"><![CDATA[<p>由于工作经常会更换机器、更换环境，有时候一个机器用惯了，换了一台机器都不记得自己之前是怎么配置的了。为了防止老年痴呆阻止我配置好看的工作环境，我决定把所有喜欢的配置都记录在这里，可能有点乱。</p><h3 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h3><p>这个应该很常用了，大家第一件事应该就是设置主题吧，我试过各种主题，都无法满足我的诉求，所以就自己配置了一下 <code>~/.jupyter/custom/custom.css</code>，感觉下面这个配置简单又好看~</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.introspection</span>, <span class="selector-class">.input_prompt</span>, <span class="selector-class">.output_prompt</span>, <span class="selector-class">.output</span>, <span class="selector-class">.CodeMirror</span> <span class="selector-tag">pre</span> &#123;</span><br><span class="line">    <span class="attribute">font-family</span>: <span class="string">"Microsoft YaHei Mono"</span>, Consolas, <span class="string">"Liberation Mono"</span>, Menlo, Courier, monospace;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">15px</span>;</span><br><span class="line">    <span class="attribute">line-height</span>: <span class="number">22px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">div</span><span class="selector-class">.output_area</span> <span class="selector-tag">pre</span> &#123;</span><br><span class="line"> <span class="attribute">font-family</span>: <span class="string">"Microsoft YaHei Mono"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">div</span><span class="selector-class">.text_cell</span>,</span><br><span class="line"><span class="selector-tag">div</span><span class="selector-class">.text_cell_render</span> <span class="selector-tag">pre</span>,</span><br><span class="line"><span class="selector-tag">div</span><span class="selector-class">.text_cell_render</span> &#123;</span><br><span class="line"> <span class="attribute">font-family</span>: sans-serif;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">11pt</span>;</span><br><span class="line"> <span class="attribute">line-height</span>: <span class="number">20pt</span>;</span><br><span class="line"> <span class="attribute">color</span>: <span class="number">#353535</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">div</span><span class="selector-class">.rendered_html</span> <span class="selector-tag">code</span> &#123;</span><br><span class="line"> <span class="attribute">font-family</span>: <span class="string">"Microsoft YaHei Mono"</span>;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">11pt</span>;</span><br><span class="line"> <span class="attribute">padding-top</span>: <span class="number">3px</span>;</span><br><span class="line"> <span class="attribute">padding-left</span>: <span class="number">6px</span>;</span><br><span class="line"> <span class="attribute">padding-right</span>: <span class="number">6px</span>;</span><br><span class="line"> <span class="attribute">color</span>: <span class="number">#a3be8c</span>;</span><br><span class="line"> <span class="attribute">background</span>: <span class="number">#efefef</span>;</span><br><span class="line"> <span class="attribute">background-color</span>: <span class="number">#efefef</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.rendered_html</span> <span class="selector-tag">thead</span> &#123;</span><br><span class="line"> <span class="attribute">font-family</span>: <span class="string">"Microsoft YaHei Mono"</span>;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">10.5pt</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.rendered_html</span> <span class="selector-tag">td</span> &#123;</span><br><span class="line"> <span class="attribute">font-family</span>: <span class="string">"Microsoft YaHei Mono"</span>;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">10pt</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.rendered_html</span> <span class="selector-tag">h1</span>,</span><br><span class="line"><span class="selector-class">.text_cell_render</span> <span class="selector-tag">h1</span> &#123;</span><br><span class="line"> <span class="attribute">color</span>: <span class="number">#126dce</span> <span class="meta">!important</span>;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">160%</span>;</span><br><span class="line"> <span class="attribute">text-align</span>: left;</span><br><span class="line"> <span class="attribute">font-style</span>: normal;</span><br><span class="line"> <span class="attribute">font-weight</span>: bold;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.rendered_html</span> <span class="selector-tag">h2</span>,</span><br><span class="line"><span class="selector-class">.text_cell_render</span> <span class="selector-tag">h2</span> &#123;</span><br><span class="line"> <span class="attribute">color</span>: <span class="number">#126dce</span> <span class="meta">!important</span>;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">140%</span>;</span><br><span class="line"> <span class="attribute">font-style</span>: normal;</span><br><span class="line"> <span class="attribute">font-weight</span>: bold;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.rendered_html</span> <span class="selector-tag">h3</span>,</span><br><span class="line"><span class="selector-class">.text_cell_render</span> <span class="selector-tag">h3</span> &#123;</span><br><span class="line"> <span class="attribute">color</span>: <span class="number">#126dce</span> <span class="meta">!important</span>;</span><br><span class="line"> <span class="attribute">font-size</span>: <span class="number">120%</span>;</span><br><span class="line"> <span class="attribute">font-style</span>: normal;</span><br><span class="line"> <span class="attribute">font-weight</span>: bold;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-class">.CodeMirror-linenumber</span> &#123;</span><br><span class="line"> <span class="attribute">font-family</span>: <span class="string">"Microsoft YaHei Mono"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.cm-s-ipython</span><span class="selector-class">.CodeMirror</span> &#123;<span class="attribute">background</span>: <span class="number">#2b303b</span>; <span class="attribute">color</span>: <span class="number">#dfe1e8</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">div</span><span class="selector-class">.CodeMirror-selected</span> &#123;<span class="attribute">background</span>: <span class="number">#343d46</span> <span class="meta">!important</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-class">.CodeMirror-gutters</span> &#123;<span class="attribute">background</span>: <span class="number">#2b303b</span>; <span class="attribute">border-right</span>: <span class="number">0px</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-class">.CodeMirror-linenumber</span> &#123;<span class="attribute">color</span>: <span class="number">#65737e</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-class">.CodeMirror-cursor</span> &#123;<span class="attribute">border-left</span>: <span class="number">1px</span> solid <span class="number">#a7adba</span> <span class="meta">!important</span>;&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-comment</span> &#123;<span class="attribute">color</span>: <span class="number">#A3BE72</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-atom</span> &#123;<span class="attribute">color</span>: <span class="number">#b48ead</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-number</span> &#123;<span class="attribute">color</span>: <span class="number">#b48ead</span>;&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-property</span>, <span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-attribute</span> &#123;<span class="attribute">color</span>: <span class="number">#c0c5ce</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-keyword</span> &#123;<span class="attribute">color</span>: <span class="number">#DDD7A3</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-string</span> &#123;<span class="attribute">color</span>: <span class="number">#94C273</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-operator</span> &#123;<span class="attribute">color</span>: <span class="number">#ab7967</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-builtin</span> &#123;<span class="attribute">color</span>: <span class="number">#EA8080</span>;&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-variable</span> &#123;<span class="attribute">color</span>: <span class="number">#c0c5ce</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-variable-2</span> &#123;<span class="attribute">color</span>: <span class="number">#8fa1b3</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-def</span> &#123;<span class="attribute">color</span>: <span class="number">#61AFEF</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-error</span> &#123;<span class="attribute">background</span>: <span class="number">#bf616a</span>; <span class="attribute">color</span>: <span class="number">#a7adba</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-bracket</span> &#123;<span class="attribute">color</span>: <span class="number">#c0c5ce</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-tag</span> &#123;<span class="attribute">color</span>: <span class="number">#bf616a</span>;&#125;</span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-tag">span</span><span class="selector-class">.cm-link</span> &#123;<span class="attribute">color</span>: <span class="number">#b48ead</span>;&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.cm-s-ipython</span> <span class="selector-class">.CodeMirror-matchingbracket</span> &#123; <span class="attribute">text-decoration</span>: underline; <span class="attribute">color</span>: <span class="number">#dfe1e8</span> <span class="meta">!important</span>;&#125;</span><br></pre></td></tr></table></figure><p>另外，在 linux 上安装 jupyter notebook 的话，第一次启动时会报 `` 的错，需要修改以下文件：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于工作经常会更换机器、更换环境，有时候一个机器用惯了，换了一台机器都不记得自己之前是怎么配置的了。为了防止老年痴呆阻止我配置好看的工作环境，我决定把所有喜欢的配置都记录在这里，可能有点乱。&lt;/p&gt;
&lt;h3 id=&quot;Jupyter-Notebook&quot;&gt;&lt;a href=&quot;#J
      
    
    </summary>
    
      <category term="安装部署" scheme="https://guyuecanhui.github.io/categories/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
      <category term="jupyter" scheme="https://guyuecanhui.github.io/tags/jupyter/"/>
    
  </entry>
  
  <entry>
    <title>例解共轭分布之视频质量评估</title>
    <link href="https://guyuecanhui.github.io/2019/08/18/conjugate-priors-video-quality/"/>
    <id>https://guyuecanhui.github.io/2019/08/18/conjugate-priors-video-quality/</id>
    <published>2019-08-18T02:47:06.000Z</published>
    <updated>2019-09-08T14:01:12.395Z</updated>
    
    <content type="html"><![CDATA[<p>在推荐领域阅读文献的时候，我们常常会遇到共轭分布、共轭先验 (<strong>conjugate prior</strong>) 之类的概念。由于共轭这个翻译实在不太直观，因此这些概念也很难理解，我想结合两个视频推荐中的例子来尝试说明这些概念。今天先介绍视频质量评估的例子。</p><h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>如何评估一个视频的质量是视频推荐中非常重要但是又很让人头疼的事情。尤其是在短视频场景下，每天新增大量的短视频，我们需要迅速判断一个新短视频的质量：如果质量很好，我们可以将它向更多的人推荐；如果质量不好，我们可能不再主动推荐该视频。这里判断的时效很重要，因为如果没有及时发现一个垃圾视频，它可能就会通过推荐系统祸害很多的用户 =.=!</p><p>由于视频非常多，我们无法人工对每个视频进行准确的质量评估，而且用户在短视频的观看行为呈现出更加多元的兴趣，因此小编们也无法代表所有用户的口味。因此，要评估一个短视频好不好，还是得看它在用户中的表现 (可以用完播率、点赞率、分享率等统计指标来度量)。</p><p>假设我们只考虑用完播率 $r$ 来度量一个视频的质量 (后面交替使用完播率和视频质量)，它表示一个视频被播放完 (或者播放超过一定比例) 的数量 $m$ 与它展现给用户的次数 $n$ 的比例：</p><script type="math/tex; mode=display">r=\frac{m}{n} \qquad(1)</script><p>这个指标的优点是计算非常方便，而且能够在一定程度上表达业务诉求。但是实际应用的时候，往往会因为视频的展示次数过少而对某个视频进行错误的评价。例如，视频 $v_1$ 只展示了 $10$ 次，有 $5$ 次完播；视频 $v_2$ 展示了 $10000$ 次，有 $4900$ 次完播，相较而言，$v_1$ 和 $v_2$ 哪个质量更好呢？</p><p>很难说！我们只是比较确信 $v_2$ 的完播率稳定在了 $49\%$ 左右，而对于 $v_1$ 的评估就非常不确定了：有可能它再展示 $10$ 次以后，一次都没人看；也有可能它再展示 $10$ 次每次都完播了。这两种情况下我们对视频质量的评估将发生非常大的变化。</p><h3 id="模型假设-先验分布"><a href="#模型假设-先验分布" class="headerlink" title="模型假设 (先验分布)"></a>模型假设 (先验分布)</h3><p>从根本上来讲，我们简单的用式 $(1)$ 来计算完播率忽略了事件过少时候的不确定性。为了引入这种不确定性，我们可以用一个概率分布来表示视频的质量 (这就是贝叶斯学派的观点，$r$ 并不是一个固定的值，而是满足一定的概率分布)，也就是说，给定 $m$ 和 $n$，我们要来估计这个视频的质量呈现一个什么样的分布。这个分布的形状是我们在看到数据之前<strong>根据经验</strong>去假定的，因此我们也叫它<strong>先验分布</strong>。</p><p>我们的直观想法是，如果一个视频的完播率为 $r=\frac{m}{n}$，那么它质量的真实分布 $\theta$ 中，概率最大的点也应该是 $\frac{m}{n}$，并且与 $\frac{m}{n}$ 相差越多概率也越小。</p><p>根据这个想法，我们可以用 <strong>Beta</strong> 分布来进行建模，将 $\alpha=m$ 和 $\beta=n-m$ 作为 <strong>Beta</strong> 分布的参数 (<strong>Beta</strong> 分布的详细介绍可以参考 Wiki)。在我们的例子中，随着 $n$ 的增加，<strong>Beta</strong> 分布的概率密度越集中于 $r=\frac{m}{n}$。下图表示随着 $m$ 和 $n$ 变化，保持 $r=0.5$ 不变的情况下，<strong>Beta</strong> 分布的概率密度函数：</p><img src="/2019/08/18/conjugate-priors-video-quality/beta-pdf.png" title="图 1. 不同参数下 Beta 分布的形状"><p>可以看到，当 $n$ 很小的时候，视频的质量是高度不确定的；而当 $n$ 很大的时候，视频的质量已经集中分布于 $r=\frac{m}{n}$ 附近了。因此，我们选的这个先验分布是能够满足我们的直观想法和假设要求的。这样，我们用式 $(2)$ 来代替式 $(1)$ 对视频质量进行初步的评估：</p><script type="math/tex; mode=display">\begin{align}p(\theta;\alpha,\beta)=Beta(\theta;\alpha,\beta)&=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\\&=\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\end{align} \qquad(2)</script><p>其中，$\Gamma(n)=(n-1)!$ 表示伽玛函数 (这个场景下参数都为整数)；$B(\alpha,\beta)$ 可以看成是归一化项，使得所有概率累加和为 $1$。</p><h3 id="更新模型-后验分布"><a href="#更新模型-后验分布" class="headerlink" title="更新模型 (后验分布)"></a>更新模型 (后验分布)</h3><p>问题才解决了一半，由于视频在不断的推荐给用户，我们的统计数据也在发生变化。因此另外一个至关重要的问题是，我们怎么根据新增的数据来更新我们对视频质量的评估。</p><p>例如，对于某个视频 $v$，假设我们已经收集到一些反馈数据，并统计出 $\alpha=m$，$\beta=n-m$，我们根据式 $(2)$ 对视频质量分布 $\theta$ 有了一个初步的估计。现在我们又将这个视频推荐给其他用户，并想看看放量以后，视频质量评估是否准确。假设这个视频又展示了 $b$ 次，完播了 $a$ 次，则我们根据先验假设，视频的质量应该是围绕 $r’=\frac{m+a}{n+b}$ 的钟形分布，并且比之前的分布更陡峭一些。为了实现这个过程，我们需要对模型进行更新。</p><p>由于用户的反馈只包含完播和未完播两类，因此很容易想到用二项分布的似然估计来估计这 $b$ 次展示中有 $a$ 次会完播的概率：</p><script type="math/tex; mode=display">p(x=a|\theta)=C_b^a\theta^{a}(1-\theta)^{(b-a)} \qquad(3)</script><p>由于式 $(3)$ 中的 $\theta$ 实际上是满足式 $(2)$ 中的分布 (<em>注意，这里 $\theta$ 虽然仍然是一个分布，但是我们在这一步假设它是已知的</em>)，代入后可以算出 $a$ 次完播的概率为 $\theta$ 取所有可能值时式 $(3)$ 的积分：</p><script type="math/tex; mode=display">p(x=a)=\int_0^1 p(x=a;\theta)p(\theta)d\theta \qquad(4)</script><p>这里我们要用最基础的贝叶斯公式，来基于初始的视频质量评估和增量收集来的统计数据，去修正我们在式 $(2)$ 中做出的视频质量评估，得到一个更加可靠的估计。贝叶斯公式如下：</p><script type="math/tex; mode=display">p(\theta;X)=\frac{p(X;\theta)p(\theta)}{p(X)} \qquad(5)</script><p>其中，$p(\theta)$ 是我们对这个视频质量的初始评估，即先验分布，用式 $(2)$ 来计算；$p(X;\theta)$ 表示我们基于初始的评估结果，进一步估计事件 $X$ 发生的概率，即似然估计，用式 $(3)$ 来计算；$p(X)$ 表示 $\theta$ 取不同值时事件 $X$ 发生的概率之和，主要是用于做归一化，用式 $(4)$ 来计算；$p(\theta;X)$ 则表示基于先验分布和似然估计，得到的后验分布。</p><p>全部代入后，我们可以得到下面的简单推导：</p><script type="math/tex; mode=display">\begin{align}p(\theta;x=a)&=\frac{p(x=a;\theta)p(\theta)}{\int_0^1 p(x=a;\theta)p(\theta)d(\theta)}\\&=\frac{C_b^a\theta^{a}(1-\theta)^{(b-a)} \frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\int_0^1 C_b^a\theta^{a}(1-\theta)^{(b-a)}\frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}d\theta}\\&=\frac{\theta^{\alpha+a-1}(1-\theta)^{\beta+b-a-1}}{\int_0^1\theta^{\alpha+a-1}(1-\theta)^{\beta+b-a-1}d\theta}\\&=\frac{1}{B(\alpha+a,\beta+b-a)}\theta^{\alpha+a-1}(1-\theta)^{\beta+b-a-1}\\&=Beta(\theta;\alpha+a,\beta+b-a)\end{align} \qquad(6)</script><p>也就是说，经过了这一轮的推荐以后，我们对这个视频的质量评估仅仅使模型的参数发了变化，而模型的形式不变，仍然为 <strong>Beta</strong> 分布！至此，我们终于触及本文的核心概念：共轭性。</p><blockquote><p> <strong>模型的先验分布与后验分布具有相同的函数形式，这个性质就叫做共轭性。</strong></p></blockquote><h3 id="共轭性"><a href="#共轭性" class="headerlink" title="共轭性"></a>共轭性</h3><p>共轭性给我们带来了什么样的好处呢？比较式 $(6)$ 和式 $(2)$，我们发现，在观察到 $b$ 次推荐中有 $a$ 次完播事件后，我们可以简单的将模型从 $Beta(\theta;\alpha,\beta)$ 更新为 $Beta(\theta;\alpha+a,\beta+b-a)$，即我们只需要更新如下模型参数：</p><script type="math/tex; mode=display">\begin{cases}\begin{align}\alpha&=\alpha+a \\\beta&=\beta+b-a\end{align}\end{cases}</script><p>它的最大意义在于简化了模型更新的过程，使得模型更新的实时性得到了保证。</p><p>一开始，我们基于先验知识对视频质量进行建模，但是由于数据量较少，我们对视频质量的估计置信度较低；随着用户反馈的数据越来越多，我们可以直接基于这些新增的数据去快速更新模型的参数；随着参数数值的增大，我们对视频质量的估计置信度越来越高，直到我们已经有充足的把握认定这个视频是不是高质量视频。</p><p>这里的置信度还体现在，当数据量较少的时候，少量的观测结果就会导致我们对视频质量评估发生巨大的变化；而当数据量充足的时候，即使再收集到很多数据，也很难改变我们的评估。</p><h3 id="Take-aways"><a href="#Take-aways" class="headerlink" title="Take-aways"></a>Take-aways</h3><p>至此，我们用视频质量评估的例子说明了共轭性和共轭分布是什么含义。一些关键点总结如下：</p><ol><li>共轭性是指模型的先验分布和后验分布有相同的形式，满足共轭性的分布称为共轭分布。例如：<strong>Beta</strong> 分布与二项分布是共轭分布，且 <strong>Beta</strong> 分布是 $\theta$ 的共轭先验；</li><li>共轭性极大的方便了我们基于增量观测的数据对模型进行更新；</li><li>在推导共轭性的时候，我们使用了贝叶斯公式，总结起来就是：后验分布=先验分布*似然函数/归一化因子；</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在推荐领域阅读文献的时候，我们常常会遇到共轭分布、共轭先验 (&lt;strong&gt;conjugate prior&lt;/strong&gt;) 之类的概念。由于共轭这个翻译实在不太直观，因此这些概念也很难理解，我想结合两个视频推荐中的例子来尝试说明这些概念。今天先介绍视频质量评估的例子。
      
    
    </summary>
    
      <category term="数学" scheme="https://guyuecanhui.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="统计" scheme="https://guyuecanhui.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
      <category term="概率" scheme="https://guyuecanhui.github.io/tags/%E6%A6%82%E7%8E%87/"/>
    
      <category term="分布" scheme="https://guyuecanhui.github.io/tags/%E5%88%86%E5%B8%83/"/>
    
      <category term="贝叶斯" scheme="https://guyuecanhui.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
      <category term="先验" scheme="https://guyuecanhui.github.io/tags/%E5%85%88%E9%AA%8C/"/>
    
      <category term="后验" scheme="https://guyuecanhui.github.io/tags/%E5%90%8E%E9%AA%8C/"/>
    
      <category term="共轭" scheme="https://guyuecanhui.github.io/tags/%E5%85%B1%E8%BD%AD/"/>
    
      <category term="Beta分布" scheme="https://guyuecanhui.github.io/tags/Beta%E5%88%86%E5%B8%83/"/>
    
      <category term="二项分布" scheme="https://guyuecanhui.github.io/tags/%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83/"/>
    
  </entry>
  
  <entry>
    <title>常用的特征选择方法之 Kendall 秩相关系数</title>
    <link href="https://guyuecanhui.github.io/2019/08/10/feature-selection-kendall/"/>
    <id>https://guyuecanhui.github.io/2019/08/10/feature-selection-kendall/</id>
    <published>2019-08-10T14:44:41.000Z</published>
    <updated>2019-08-17T14:46:14.499Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们已经讨论了 <a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/"><strong>Pearson</strong> 相关系数</a>和 <a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-spearman/"><strong>Spearman</strong> 秩相关系数</a>，它们可以检测连续变量间的相关性，并且 <strong>Spearman</strong> 秩相关系数还能够检测有序的离散变量间的相关系数。今天我们再讨论一个能够检测有序变量相关性的系数：<strong>Kendall</strong> 秩相关系数。这里有序变量既包括实数变量，也包括可以排序的类别变量，比如名次、年龄段等。</p><h3 id="Kendall-秩相关系数的定义"><a href="#Kendall-秩相关系数的定义" class="headerlink" title="Kendall 秩相关系数的定义"></a>Kendall 秩相关系数的定义</h3><p><strong>Kendall</strong> 秩相关系数是一个非参数性质（与分布无关）的秩统计参数，是用来度量两个<strong>有序变量</strong>之间<strong>单调关系</strong>强弱的相关系数，它的取值范围是 $[-1,1]$，绝对值越大，表示单调相关性越强，取值为 $0$ 时表示完全不相关。</p><p>原始的 <strong>Kendall</strong> 秩相关系数定义在<strong>一致对</strong> (<strong>concordant pairs</strong>) 和<strong>分歧对</strong> (<strong>discordant pairs</strong>) 的概念上。所谓一致对，就是两个变量取值的相对关系一致；分歧对则是指它们的相对关系不一致。这么说有点难以理解，我们举个例子。</p><p>假设我们为很多不同年龄的用户推送了一条社保相关的视频，然后回收了这些用户的播放完成度，如下表所示：</p><img src="/2019/08/10/feature-selection-kendall/age-value-pairs.png" title="图 1. 用户年龄与播放完成度的关系"><p>我们想用 <strong>Kendall</strong> 秩相关系数来分析用户年龄与该社保视频的播放情况是否相关。为此，我们将年龄和播放完成度分别排序后，对样本中取值进行排序和编号，分别得到 <code>年龄序号</code> 和 <code>播放序号</code>。这时，对于样本 $3$ 和样本 $4$，它们的年龄序号是 $[3,4]$，播放序号是 $[2,4]$，虽然序号不同，但是变化趋势是相同的，因此它们是一致的；对于样本 $2$ 和样本 $3$，它们的年龄序号是 $[2,3]$，播放序号是 $[5,2]$，它们的变化趋势是相反的，因此它们是分歧的。</p><p>进一步的，我们观察可以发现，当样本已经按年龄升序排列后，对于每个样本，我们可以简单的数一下该样本后续样本中播放序号大于该样本的样本数量，作为该样本引入的一致对数 (该样本之前的样本与该样本也可能一致，但是已经算过一次了)，将所有样本引入的一致对数加起来就能得到所有样本的一致对数，记为 $c$。</p><p>同样的，对于每个样本，我们可以简单的数一下该样本后续样本中播放序号小于该样本的样本数量，作为该样本引入的分歧对数，累加后得到所有样本的分歧对数，记为 $d$。</p><p>则原始的 <strong>Kendall</strong> 秩相关系数定义为：</p><script type="math/tex; mode=display">\tau_a=\frac{c-d}{c+d}=\frac{c-d}{\frac{1}{2}\cdot n\cdot (n-1)}\qquad (1)</script><p>其中，$m=\frac{n\cdot (n-1)}{2}$ 表示所有样本两两组合的数量，在变量没有重复取值的情况下，$m=c+d$。定义 $(1)$ 也被称为 <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" target="_blank" rel="noopener"><strong>Tau-a</strong></a>，从定义也容易看出，它不能处理变量有相同取值的情况。</p><p>为了处理变量有相同取值的情况，我们还要将每个变量中相同取值的数量考虑进来，从而得到扩展的定义：</p><script type="math/tex; mode=display">\tau_b=\frac{c-d}{\sqrt{(c+d+t_x)(c+d+t_y)}}\qquad (2)</script><p>其中，$c$ 在计算的时候只能算 <script type="math/tex">a_i<a_j</script> 且 <script type="math/tex">b_i<b_j</script> 的对数，$d$ 也只能算 <script type="math/tex">a_i<a_j</script> 且 <script type="math/tex">b_i>b_j</script> 的对数 (<script type="math/tex">i<j</script>)；$t_x$，$t_y$ 分别表示变量 $x$，$y$ 取值中序号相同的样本对数排除共同平局的部分 (在下一小节举例说明)。式 <script type="math/tex">(2)</script> 通常又被称为 <strong>Tau-b</strong>，是实际中应用最广泛的定义 (另外还有 <strong>Tau-c</strong> 的变种这里就不介绍了)。在 <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html" target="_blank" rel="noopener"><strong>scipy 1.3.0</strong></a> 版本的实现中，同时支持式 $(1)$ 和式 $(2)$。</p><h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>首先，我们根据式 $(1)$ 算一下图 $1$ 中年龄与播放的相关度。</p><ol><li>将样本按年龄升序排列，将播放完成度按从小到大的顺序编号，如图 $1$ 所示；</li><li>分别计算每个样本新引入的一致对数和分歧对数，如图 $1$ 所示，进而算出 $c=40$，$d=5$；</li><li>根据式 $(1)$ 得到 $\tau_a=\frac{40-5}{40+5}=0.778$；</li></ol><p>因此，年龄与播放社保视频的时长呈现强相关性，基于这个分析我们就可以尝试对更多年龄大一些的用户推送此视频。</p><p>在工程实现的时候，用户的年龄通常会被划分成不同的区间，而播放完成度只有超过一定阈值 (如 $0.3$) 我们才算作有效播放。因此，图 $1$ 的数据我们又可以转换成下面的离散情况：</p><img src="/2019/08/10/feature-selection-kendall/age-discrete-pairs.png" title="图 2. 离散化的用户年龄与播放完成度的关系"><p>可以发现，年龄段序号和有效播放序号存在大量的重复数据，因此我们基于式 $(2)$ 来计算：</p><ol><li>将样本按年龄段升序排列，相同的年龄段按是否有效播放排序，对年龄段和是否有效播放进行编号，如图 $2$ 所示；</li><li>计算每个样本引入的一致对数和分歧对数，如图 $2$ 所示 (例如样本 $4$ 与 样本 $8\sim 10$ 一致)，进而算出 $c=21$，$d=0$；</li><li>计算公共平局的数量 $t_c$，公共平局是指 $a_i=a_j$ 且 $b_i=b_j$ 的情况 (例如样本 $1\sim 3$ 互为平局，样本 $4,5,7$ 互为平局，样本 $8,9$ 互为平局)，根据图 $2$ 易知：$t_c=\frac{3\cdot (3-1)}{2}+\frac{3\cdot (3-1)}{2}+\frac{2\cdot (2-1)}{2}=7$；</li><li>计算只在年龄段平局的数量 $t_x=\frac{3\cdot (3-1)}{2}+\frac{4\cdot (4-1)}{2}+\frac{2\cdot (2-1)}{2}-t_c=10-7=3$；</li><li>计算只在有效播放平均局的数量 $t_y=\frac{6\cdot (6-1)}{2}+\frac{4\cdot (4-1)}{2}-t_c=21-7=14$；</li><li>根据式 $(2)$ 得到 $\tau_b=\frac{21}{\sqrt{(21+3)(21+14)}}=0.725$；</li></ol><p>对比发现，离散化后，我们发现这两个因素之间仍然是强相关的。</p><blockquote><p>附示例的 python 代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> kendalltau</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=[<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kendalltau(x,y)</span><br><span class="line">(<span class="number">0.7777777777777779</span>, <span class="number">0.0017451191944018172</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>kendalltau(x,y)</span><br><span class="line">(<span class="number">0.72456883730947197</span>, <span class="number">0.0035417200011750309</span>)</span><br></pre></td></tr></table></figure></p><p>其中，<code>kendalltau</code> 返回的第二个结果是 p-value，其具体含义可参考<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html" target="_blank" rel="noopener">官方文档</a>。</p></blockquote><h3 id="Take-aways"><a href="#Take-aways" class="headerlink" title="Take-aways"></a>Take-aways</h3><ol><li><strong>Kendall</strong> 秩相关系数可以用于度量有序变量间相关性，只要求变量取值之间可比，对变量的分布和数据的距离不作假设；</li><li>能用 <strong>Pearson</strong> 相关系数和 <strong>Spearman</strong> 秩相关系数的地方都能用 <strong>Kendall</strong> 秩相关系数，但是 <strong>Spearman</strong> 和 <strong>Kendall</strong> 秩相关系数要对数据排序，复杂度远高于 <strong>Pearson</strong> 相关系数，因此能用 <strong>Pearson</strong> 相关系数的时候优先考虑 <strong>Pearson</strong> 相关系数；</li><li><strong>Kendall</strong> 秩相关系数依赖一致对和分歧对的计数，这里需要注意数据中是否有重复取值的情况，来选择使用 <strong>Tau-a</strong> 还是 <strong>Tau-b</strong> 进行计算。</li></ol><hr><blockquote><h4 id="这是特征选择系列文章的第三篇，其他文章可参考："><a href="#这是特征选择系列文章的第三篇，其他文章可参考：" class="headerlink" title="这是特征选择系列文章的第三篇，其他文章可参考："></a>这是特征选择系列文章的第三篇，其他文章可参考：</h4><ol><li><a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">常用的特征选择方法之 Pearson 相关系数</a></li><li><a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-spearman/">常用的特征选择方法之 Spearman 相关系数</a></li><li><a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-kendall/">常用的特征选择方法之 Kendall 秩相关系数</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面我们已经讨论了 &lt;a href=&quot;https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/&quot;&gt;&lt;strong&gt;Pearson&lt;/strong&gt; 相关系数&lt;/a&gt;和 &lt;a href=&quot;https
      
    
    </summary>
    
      <category term="特征工程" scheme="https://guyuecanhui.github.io/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81/"/>
    
      <category term="特征选择" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="特征过滤" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4/"/>
    
      <category term="Kendall" scheme="https://guyuecanhui.github.io/tags/Kendall/"/>
    
  </entry>
  
  <entry>
    <title>常用的特征选择方法之 Spearman 秩相关系数</title>
    <link href="https://guyuecanhui.github.io/2019/07/28/feature-selection-spearman/"/>
    <id>https://guyuecanhui.github.io/2019/07/28/feature-selection-spearman/</id>
    <published>2019-07-28T02:36:03.000Z</published>
    <updated>2019-08-17T14:18:27.424Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">上一篇</a>里，我们简单的介绍了基于 <strong>Pearson</strong> 相关系数的特征选择方法，本篇介绍另一种使用更加广泛的相关系数：<strong>Spearman</strong> 秩相关系数，简称 <strong>Spearman</strong> 相关系数。<strong>Spearman</strong> 相关系数与 <strong>Pearson</strong> 相关系数、<strong>Kendall</strong> 相关系数并称统计学三大相关系数，足见其重要性。</p><p>有了 <strong>Pearson</strong> 相关系数，为什么还要用 <strong>Spearman</strong> 相关系数呢，主要是 <strong>Pearson</strong> 系数只能度量两个服从正态分布的变量之间线性相关性的强弱 (如果不熟悉可以回顾一下上一篇的介绍)，而 <strong>Spearman</strong> 系数只度量<strong>单调关系</strong>，而不考虑具体数值的影响，因此 <strong>Spearman</strong> 相关系数的应用范围更广，不仅对数据分布不作任何假设，能够容忍异常值，也不需要数据的取值是等距的（例如比赛中，第 1 名和第 2 名的距离与第 2 名和第 3 名的距离是不等的），因此除非是考虑性能的影响，能用 <strong>Pearson</strong> 系数的地方都能用 <strong>Spearman</strong> 系数。</p><h3 id="Spearman-秩相关系数的定义"><a href="#Spearman-秩相关系数的定义" class="headerlink" title="Spearman 秩相关系数的定义"></a>Spearman 秩相关系数的定义</h3><p><a href="https://blog.csdn.net/liuyuan_jq/article/details/52542211" target="_blank" rel="noopener"><strong>Spearman</strong> 秩相关系数</a>是一个非参数性质（与分布无关）的秩统计参数，是用来度量两个<strong>连续型变量</strong>之间<strong>单调关系</strong>强弱的相关系数，取值范围也是 $[-1,1]$。在没有重复数据的情况下，如果一个变量是另外一个变量的严格单调函数，则 <strong>Spearman</strong> 秩相关系数就是 $1$ 或 $-1$，称变量完全 <strong>Spearman</strong> 秩相关。</p><p>这里的秩相关 (<strong>Rank Correlation</strong>)，又称等级相关，是将两变量的样本值按数据的大小顺序排列位次，以各要素样本值的位次代替实际数据而求得的一种统计量。排序不论从大到小还是从小到大排都无所谓，只要保证大家排序的标准一致即可。</p><p>用 $\rho_s$ 来表示 <strong>Spearman</strong> 相关系数 (用 $\rho_p$ 表示 <strong>Pearson</strong> 相关系数)。如果每个变量都没有相同的取值 (即没有相同的秩次)，则 <strong>Spearman</strong> 相关系数可由下式计算：</p><script type="math/tex; mode=display">\rho_s=1-\frac{6\sum{d_i^2}}{n(n^2-1)}</script><p>其中，$n$ 表示数据点的个数；<script type="math/tex">d_i</script> 表示数据点 <script type="math/tex">(x_i,y_i)</script> 的秩次 <script type="math/tex">(r_{x_i},r_{y_i})</script> 之差：<script type="math/tex">d_i=r_{x_i}-r_{y_i}</script>。</p><p>如果某个变量有重复数据，则计算变量之间的 <strong>Spearman</strong> 相关系数就是计算变量数据秩次之间的 <strong>Pearson</strong> 相关系数：</p><script type="math/tex; mode=display">\rho_s=\rho_{r_x,r_y}=\frac{\text{cov}(r_x,r_y)}{\sigma_{r_x}\sigma_{r_y}}</script><p>其中，$r_x$ 表示变量 $\boldsymbol{x}$ 转换后的秩次。从这个定义可以看出来，<strong>Spearman</strong> 相关系数实际上就是对数据做了秩次变换后的 <strong>Pearson</strong> 相关系数。</p><h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>我们还是拿上一篇的例子来说明。首先将样本进行秩次变换，样本升序排列后的位次如图 1 所示：</p><img src="/2019/07/28/feature-selection-spearman/rank-correlation.png" title="图 1. 将变量 $x$, $y$ 用其排序的位次 $r_x$, $r_y$ 来代替"><p>需要说明的是，这里变量 $y$ 有两个重复数据 $0.1$，在排序的时候它们的位次相同，此时可以用相同位次的数据所占的位次之和除以数据的数量 (即 $\frac{1+2}{2}=1.5$) 来作为这些重复数据的位次。</p><p>根据定义，当存在重复数据的时候，我们计算秩次 (即 $r_x$, $r_y$) 的 <strong>Pearson</strong> 相关系数 (过程省略)，得到结果 $\rho_s=0.994$，几乎是单调相关了，其数值比直接计算原始数据的 <strong>Pearson</strong> 相关系数 $\rho_p=0.972$ 还要大一些。</p><p>实际上，当 <strong>Pearson</strong> 相关系数比较大的时候，<strong>Spearman</strong> 相关系数也比较大；而当 <strong>Pearson</strong> 相关系数比较小的时候，<strong>Spearman</strong> 相关系数仍然可能较大，例如变量之间是指数相关 ($y=e^x$，如图 2 所示) 时，它们的 <strong>Pearson</strong> 相关系数和 <strong>Spearman</strong> 相关系数分别是 $0.7758$ 和 $1.0$。</p><img src="/2019/07/28/feature-selection-spearman/pearson-vs-spearman.png" title="图 2. 变量 $x$, $y$ 之间的 Pearson 相关系数为 $0.7758$，Spearman 相关系数为 $1.0$"><p>最后，我们看看<a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">上一篇图 3</a> 所示的异常数据对 <strong>Spearman</strong> 相关系数的影响，引入异常点 $(0.9,-1.0)$ 后，变量 $x$, $y$ 的 <strong>Pearson</strong> 相关系数降为了 $\rho_p=-0.0556$，它们的 <strong>Spearman</strong> 相关系数也受到了较大的影响，降到了 $\rho_s=0.3234$，也就是较弱的正相关性。但是从这个例子仍然可以看出，与 <strong>Pearson</strong> 相关系数相比，<strong>Spearman</strong> 相关系数对异常值容忍度更高一些。</p><blockquote><p>附示例的 python 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> spearmanr, pearsonr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">1.0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>spearmanr(x,y)</span><br><span class="line">(<span class="number">0.99402979738800479</span>, <span class="number">5.2961535156451228e-07</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rx=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ry=[<span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pearsonr(rx,ry)</span><br><span class="line">(<span class="number">0.99402979738800501</span>, <span class="number">5.2961535156445373e-07</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">-1.0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>spearmanr(x,z)</span><br><span class="line">(<span class="number">0.32335909071657992</span>, <span class="number">0.43463944855085729</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z=[<span class="number">0.1</span>, <span class="number">0.12</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">-1.0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>spearmanr(x,z)</span><br><span class="line">(<span class="number">0.32335909071657992</span>, <span class="number">0.43463944855085729</span>)</span><br></pre></td></tr></table></figure><p>这里，<code>spearmanr</code> 返回的第二个结果是 p-value，其具体含义可参考<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html" target="_blank" rel="noopener">官方文档</a>。</p></blockquote><h3 id="Take-aways"><a href="#Take-aways" class="headerlink" title="Take-aways"></a>Take-aways</h3><p>本文简单介绍了 Spearman 相关系数，主要注意点总结如下：</p><ol><li><strong>Spearman</strong> 相关系数是度量两个<strong>连续型变量</strong>之间<strong>单调关系</strong>强弱的相关系数，它对数据的分布不作任何假设，能够容忍异常值，也不需要数据的取值是等距的；</li><li><strong>Spearman</strong> 相关系数实际上就是对数据做了秩次变换后的 <strong>Pearson</strong> 相关系数，只要能用 <strong>Pearson</strong> 相关系数的地方就能使用 <strong>Spearman</strong> 相关系数；</li><li><strong>Spearman</strong> 相关系数还需要对原始数据进行排序，因此计算复杂度高于 <strong>Pearson</strong> 相关系数，当数据满足 <strong>Pearson​</strong> 相关系数的使用条件时，优先考虑使用 <strong>Pearson</strong> 相关系数。</li></ol><hr><blockquote><h4 id="这是特征选择系列文章的第二篇，其他文章可参考："><a href="#这是特征选择系列文章的第二篇，其他文章可参考：" class="headerlink" title="这是特征选择系列文章的第二篇，其他文章可参考："></a>这是特征选择系列文章的第二篇，其他文章可参考：</h4><ol><li><a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">常用的特征选择方法之 Pearson 相关系数</a></li><li><a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-spearman/">常用的特征选择方法之 Spearman 相关系数</a></li><li><a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-kendall/">常用的特征选择方法之 Kendall 秩相关系数</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/&quot;&gt;上一篇&lt;/a&gt;里，我们简单的介绍了基于 &lt;strong&gt;Pearson&lt;/strong&gt; 相关系数的特征选择方法，本篇
      
    
    </summary>
    
      <category term="特征工程" scheme="https://guyuecanhui.github.io/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81/"/>
    
      <category term="特征选择" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="特征过滤" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4/"/>
    
      <category term="Spearman" scheme="https://guyuecanhui.github.io/tags/Spearman/"/>
    
  </entry>
  
  <entry>
    <title>常用的特征选择方法之 Pearson 相关系数</title>
    <link href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/"/>
    <id>https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/</id>
    <published>2019-07-20T14:36:03.000Z</published>
    <updated>2019-08-17T14:18:20.347Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，特征选择是机器学习活动至关重要的一步。最理想的情况下，我们把所有影响目标的独立因素给找出来，然后使用合适的量化手段，就能够得到完美描述目标问题的特征列表，用这些特征去建立合适容量的模型，这样的模型能够完美的匹配我们要解决的任务。</p><p>但是实际上这种想法太难实现了，我们往往只能从已有的数据出发，通过一些特征变换和组合得到一些原始特征，然后从这些原始特征中选出与目标相关的特征。</p><p>随着深度网络的崛起，越来越多的未经复杂变换的原始特征被加入到了深度网络中，大家期待有用的特征能够被自动的抽取和组合出来。但是这并不意味着特征工程就不需要了，推荐系统的大牛 Xavier 在技术博客《Rules of Machine Learning: Best Practices for ML Engineering》中提到很多关于特征工程的建议，非常值得一读，其中包含的思想就是特征是随着系统的优化进程而逐步添加的，并非一蹴而就，要始终保证特征的简单、直观、可复用、可监控和可靠性，这意味着我们需要时常对系统中存量特征做测试和筛选。</p><p>特征选择通常有过滤法（Filter）、打包法（Wrap）和嵌入法（Embed），其中，后两者都是与模型相关的，需要具体问题具体对待，而过滤法是指对特征进行预处理，提前过滤掉一些对目标无益（即对模型无益）的特征，它只考虑任务目标，而与模型无关。</p><p>我打算把常用的特征选择方法都再回顾一遍，力争把每种方法都讲得通俗易懂。这篇文章先介绍 <strong>Pearson</strong> 相关系数。</p><h3 id="Pearson-相关系数的定义"><a href="#Pearson-相关系数的定义" class="headerlink" title="Pearson 相关系数的定义"></a>Pearson 相关系数的定义</h3><p><strong>Pearson</strong> 相关系数是用来检测两个<strong>连续型变量</strong>之间<strong>线性相关</strong>的程度，取值范围为 $[-1,1]$，正值表示正相关，负值表示负相关，绝对值越大表示线性相关程度越高。在实际做特征工程时候，如果两个变量的相关系数取值为负，可以将特征变量取负号，使之与目标变量正相关，这样来保证所有特征与目标之间都是正相关。</p><p>两个变量之间的 <strong>Pearson</strong> 相关系数定义为两个变量之间的协方差和标准差的商：</p><script type="math/tex; mode=display">\rho_{\boldsymbol{x},\boldsymbol{y}}=\frac{\text{cov}(\boldsymbol{x},\boldsymbol{y})}{\sigma_\boldsymbol{x}\sigma_\boldsymbol{y}}=\frac{E[(\boldsymbol{x}-\mu_\boldsymbol{x},\boldsymbol{y}-\mu_\boldsymbol{y})]}{\sigma_\boldsymbol{x}\sigma_\boldsymbol{y}} \qquad(1)</script><p>上式定义了<strong>总体</strong>相关系数，常用希腊小写字母 $\rho$ 作为代表符号。估算样本的协方差和标准差，可得到<strong>样本 Pearson 相关系数</strong>，用英文小写字母 $r$ 表示：</p><script type="math/tex; mode=display">r_{\boldsymbol{x},\boldsymbol{y}}=\frac{\sum ^n _{i=1}(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum ^n _{i=1}(x_i - \overline{x})^2} \sqrt{\sum ^n _{i=1}(y_i - \overline{y})^2}} \qquad(2)</script><p>记 $\boldsymbol{x}’=\boldsymbol{x}-\overline{x}$ 和 $\boldsymbol{y}’=\boldsymbol{y}-\overline{y}$ 表示对变量 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 进行 $0$ 均值化，则实际上 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 的 <strong>Pearson</strong> 相关系数就是 $\boldsymbol{x}’$ 和 $\boldsymbol{y}’$ 的 <strong>cosine</strong> 相似度：$r_{\boldsymbol{x},\boldsymbol{y}}=\cos(\boldsymbol{x}’,\boldsymbol{y}’)=\frac{\boldsymbol{x}’\cdot\boldsymbol{y}’}{|\boldsymbol{x}’|\cdot|\boldsymbol{y}’|}$。</p><h3 id="Pearson-相关系数的使用条件"><a href="#Pearson-相关系数的使用条件" class="headerlink" title="Pearson 相关系数的使用条件"></a>Pearson 相关系数的使用条件</h3><p>使用 <strong>Pearson</strong> 相关系数之前需要检查数据是否满足前置条件：</p><ol><li>两个变量间有线性关系；</li><li>变量是连续变量；</li><li>变量均符合正态分布，且二元分布也符合正态分布；</li><li>两变量独立；</li><li>两变量的方差不为 0；</li></ol><p>这些条件在实际中很容易被忽略。</p><p>例如，在视频推荐中，我们可以将用户对视频的播放完成度作为目标变量，检测其他连续型特征与它的相关性，或者将这些连续型特征做特定的变换后，检测其与播放完成度的相关性。</p><p>但是播放完成度实际上不是正态分布的，如下图所示（实际上大多数日志统计特征，如用户播放视频数、视频播放完成度等，也都不服从正态分布），因此实际上是不能使用 <strong>Pearson</strong> 相关系数的，这时候可以用 <strong>Spearman</strong> 或者 <strong>Kendall</strong> 相关系数来代替。</p><img src="/2019/07/20/feature-selection-pearson/play-ratio-distribution.png" title="图 1. 视频播放完成度分布"><p>另外要注意的是，如果两个变量本身就是线性的关系，那么 <strong>Pearson</strong> 相关系数绝对值越大相关性越强，绝对值越小相关性越弱；但在当两个变量关系未知情况下，<strong>Pearson</strong> 相关系数的大小就没有什么指导意义了，它的绝对值大小并不能表征变量间的相关性强弱，这个时候最好能够画图出来看看作为辅助判断。我会在下面的例子里再详细的说明这一点。</p><h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>我们举个例子来看如何计算 <strong>Pearson</strong> 相关系数（这里仅仅演示计算过程，实际上数据的分布也不满足使用 <strong>Pearson</strong> 相关系数的条件）。</p><p>考虑视频推荐场景下，假设我们的目标 (之一) 是最大化视频的播放完成度 $y$，播放完成度的取值范围是 $[0,1]$，我们需要分析哪些因素跟 $y$ 相关，例如有一维特征是表示用户对视频的偏好度，记为 $x$，它的取值范围也是 $[0,1]$，我们把几条样本中 $x$ 和 $y$ 的取值计算出来，并画成散点图，如下所示：</p><img src="/2019/07/20/feature-selection-pearson/ratio-preference.png" title="图 2. 用户对视频的偏好度与播放完成度的对应关系"><p>我们可以按照公式 (2) 来计算 $x$ 与 $y$ 的 <strong>Pearson</strong> 相关系数：</p><ol><li>计算变量平均值：$\overline{x} = 0.5,\ \overline{y}=0.55$；</li><li>计算平移后的变量：$\boldsymbol{x}=[-0.4,-0.3,-0.2,-0.1,0.1,0.2,0.3,0.4]$，$\boldsymbol{y}=[-0.45,-0.45,-0.35,0.05,0.15,0.25,0.35,0.45]$；</li><li>计算公式 (2) 的结果：$r=\frac{0.73}{\sqrt{0.6}\cdot\sqrt{ 0.94}}=0.972$； </li></ol><p>通过计算，我们发现，这个特征与目标变量之间的线性相关性非常高，这与我们看图得到的认知是一致的。因此我们可以把这一维特征作为有效特征加入。</p><p>但是，如果我们对这个例子稍加修改，将最后一个数据点 $(0.9,1.0)$ 改为 $(0.9,-1.0)$，如图 3 所示：</p><img src="/2019/07/20/feature-selection-pearson/ratio-preference-abnormal.png" title="图 3. 引入异常数据对 Pearson 相关系数的影响"><p>从我们的观察来看，最后一个数据点可能是噪声或者异常值，对我们判断两个变量的线性相关性应该不造成影响，但是实际上，我们再次计算一下这两个变量的 <strong>Pearson</strong> 相关系数，此时的值仅仅只有 $-0.0556$，可以说是几乎不线性相关了，这说明 <strong>Pearson</strong> 相关系数小并不代表线性相关性一定弱。在这种情况下，我们应该在数据清洗阶段把特征的异常值过滤或者平滑掉以后，再计算它与目标的相关系数。</p><p>反过来，<strong>Pearson</strong> 相关系数大也并不代表线性相关性一定强。<a href="https://en.wikiversity.org/wiki/Correlation" target="_blank" rel="noopener">图 4</a> 列举了几个 <strong>Pearson</strong> 相关系数均为 $0.816$ 的变量数据，其中有些变量间并非明显的线性相关，或者是明显的二次相关，只是 <strong>Pearson</strong> 相关系数恰好较大而已。</p><img src="/2019/07/20/feature-selection-pearson/different-shape-of-same-pearson.png" title="图 4. 几组 Pearson 相关系数为 $0.816$ 的数据"><blockquote><p>附示例的 python 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">1.0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pearsonr(x, y)</span><br><span class="line">(<span class="number">0.97203814535663591</span>, <span class="number">5.3516208203873684e-05</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">-1.0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pearsonr(x, z)</span><br><span class="line">(<span class="number">-0.055618651039326214</span>, <span class="number">0.89592989552025337</span>)</span><br></pre></td></tr></table></figure><p>这里，<code>pearsonr</code> 返回的第二个结果是 p-value，其具体含义可参考<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html" target="_blank" rel="noopener">官方文档</a>。</p></blockquote><h3 id="Take-aways"><a href="#Take-aways" class="headerlink" title="Take-aways"></a>Take-aways</h3><p>本文简单的介绍了基于 <strong>Pearson</strong> 相关系数的特征选择方法，主要注意点总结如下：</p><ol><li><strong>Pearson</strong> 相关系数是用来检测两个<strong>连续型变量</strong>之间<strong>线性相关</strong>的程度，并且要求这两个变量分别分布服从正态分布；</li><li><strong>Pearson</strong> 相关系数仅能度量变量间的线性相关性，如果变量间相关性未知，则 <strong>Pearson</strong> 相关系数的大小没有指导意义，此时需要借助可视化手段辅助判断；</li><li>两变量的 <strong>Pearson</strong> 相关系数实际上是这两个变量 $0$ 均值化后的 <strong>cosine</strong> 相似度；</li><li>如果两个变量是非线性相关，为了使用线性模型，可以先将特征变量进行非线性变换，使之与目标线性相关；</li><li><strong>Pearson</strong> 相关系数对异常值比较敏感，在数据清洗阶段需要将异常值过滤或者平滑处理。</li></ol><hr><blockquote><h4 id="这是特征选择系列文章的第一篇，其他文章可参考："><a href="#这是特征选择系列文章的第一篇，其他文章可参考：" class="headerlink" title="这是特征选择系列文章的第一篇，其他文章可参考："></a>这是特征选择系列文章的第一篇，其他文章可参考：</h4><ol><li><a href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/">常用的特征选择方法之 Pearson 相关系数</a></li><li><a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-spearman/">常用的特征选择方法之 Spearman 相关系数</a></li><li><a href="https://guyuecanhui.github.io/2019/07/28/feature-selection-kendall/">常用的特征选择方法之 Kendall 秩相关系数</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;众所周知，特征选择是机器学习活动至关重要的一步。最理想的情况下，我们把所有影响目标的独立因素给找出来，然后使用合适的量化手段，就能够得到完美描述目标问题的特征列表，用这些特征去建立合适容量的模型，这样的模型能够完美的匹配我们要解决的任务。&lt;/p&gt;
&lt;p&gt;但是实际上这种想法太
      
    
    </summary>
    
      <category term="特征工程" scheme="https://guyuecanhui.github.io/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81/"/>
    
      <category term="特征选择" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="特征过滤" scheme="https://guyuecanhui.github.io/tags/%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4/"/>
    
      <category term="Pearson" scheme="https://guyuecanhui.github.io/tags/Pearson/"/>
    
  </entry>
  
  <entry>
    <title>用 FTRL 训练 FM 模型</title>
    <link href="https://guyuecanhui.github.io/2019/07/03/ftrl-fm/"/>
    <id>https://guyuecanhui.github.io/2019/07/03/ftrl-fm/</id>
    <published>2019-07-03T14:45:49.000Z</published>
    <updated>2019-07-03T14:28:24.710Z</updated>
    
    <content type="html"><![CDATA[<p>近期尝试了基于 <strong>FTRL</strong> 来训练 <strong>FM</strong> 模型，用于短视频的排序。这篇博客主要总结一下算法的理论推导和工程化的一些心得。</p><h2 id="一、FM-Factorization-Machines-模型推导"><a href="#一、FM-Factorization-Machines-模型推导" class="headerlink" title="一、FM (Factorization Machines) 模型推导"></a>一、FM (Factorization Machines) 模型推导</h2><h3 id="FM-模型简介"><a href="#FM-模型简介" class="headerlink" title="FM 模型简介"></a>FM 模型简介</h3><p>在设计排序模型时，至关重要的步骤就是特征的构造和选择。除了一些简单单特征外，往往要对特征进行组合，例如对用户的年龄、性别组合，对视频的演员、类别进行组合等，更大的特征空间能够增加模型表征能力。对于特征组合来说，业界现在通用的做法主要有两大类：</p><ul><li><strong>FM</strong> 系列，常见的模型包括 <strong>FM</strong>，<strong>FFM</strong>，<strong>DeepFM</strong>，它们对特征的取值范围比较敏感。</li><li><strong>Tree</strong> 系列，常见的模型包括 <strong>GBDT</strong>，它们对特征的取值范围不敏感。</li></ul><p>其中，<strong>FM</strong> 系列由于适合处理大规模稀疏数据，并且易于与深度神经网络结合，因此使用十分广泛，成为大厂居家必备。</p><p><strong>FM</strong> 模型的主要思想是在 <strong>LR</strong> 的基础上，对所有的特征自动做两两组合$^{[1,2]}$。两两组合最直观的方法就是为每对特征组合设置一个参数（例如 <strong>Poly2</strong> 模型），但是这样就需要 $\text{O}(n^2)$ 个参数，当特征数量很多时，需要的样本量也是巨大的，往往不可能所有的参数都有充足的样本训练。因此 <strong>FM</strong> 考虑使用矩阵分解的方式来还原这个 $n\times n$ 的参数矩阵，只需要 $n\times k$ （$k$ 通常是个很小的常数）的参数即可实现特征两两组合的目的。 </p><p>具体来说，给定样本 $z=(\boldsymbol{x},y)$，记 $\boldsymbol{v}_i = (v_i^{(1)},\cdots,v_i^{(d)})^\top$ 为第 $i$ 维特征对应的隐式向量，则 <strong>FM</strong> 模型为：</p><script type="math/tex; mode=display">\begin{align}f(\boldsymbol{x}|\boldsymbol{w})&=w_0+\sum_{i=1}^n w_ix_i+\sum_{i=1}^{n}\sum_{j=i+1}^{n} (\boldsymbol{v}_i^\top \boldsymbol{v}_j)x_ix_j \\&=w_0+\sum_{i=1}^n w_ix_i+\frac{1}{2}\Big(\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{d} v_i^{(k)} v_j^{(k)}x_ix_j- \sum_{i=1}^{n}\sum_{k=1}^{d} (v_i^{(k)}x_i)^2\Big) \\&=w_0+\sum_{i=1}^n w_ix_i+\frac{1}{2}\sum_{k=1}^{d}\Big(\sum_{i=1}^{n}v_i^{(k)} x_i\sum_{j=1}^{n} v_j^{(k)} x_j- \sum_{i=1}^{n} (v_i^{(k)}x_i)^2\Big) \\&=w_0+\sum_{i=1}^n w_ix_i+\frac{1}{2}\sum_{k=1}^{d}\Big(\big(\sum_{i=1}^{n}v_i^{(k)} x_i \big)^2- \sum_{i=1}^{n} (v_i^{(k)}x_i)^2\Big)\end{align} \qquad (1)</script><p><strong>FM</strong> 的参数包括 $\boldsymbol{w}={w_0,\cdots w_n,v_1^{(1)},\cdots v_n^{(d)}}$，容易得到 <strong>FM</strong> 对各参数的偏导如下：</p><script type="math/tex; mode=display">\frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w}=\begin{cases}\begin{align}1 &, \qquad w=w_0 \\x_i &, \qquad w=w_i,\ i=1,\cdots,n \\x_i\Big(\sum_{j=1}^n v_j^{(k)}x_j - v_i^{(k)}x_i\Big) &, \qquad w=v_i^{(k)},\ i=1,\cdots,n;\ k=1,\cdots,d\end{align}\end{cases} \qquad (2)</script><h3 id="FM-模型求解（回归问题）"><a href="#FM-模型求解（回归问题）" class="headerlink" title="FM 模型求解（回归问题）"></a>FM 模型求解（回归问题）</h3><p>此时直接将 $\hat{y} = f(\boldsymbol{x}|\boldsymbol{w})$ 作为对 $y$ 的预测结果，因此可以将样本 $z=(\boldsymbol{x},y)$ 的损失函数定义为：</p><script type="math/tex; mode=display">l(\boldsymbol{w},z) = \big(\hat{y}-y\big)^2= \big(f(\boldsymbol{x}|\boldsymbol{w})-y\big)^2 \qquad(3)</script><p>损失函数对参数的偏导为：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial l(\boldsymbol{w},z)}{\partial w} &= 2\big(\hat{y}-y\big)\cdot \frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w} \\&= 2 \big(f(\boldsymbol{x}|\boldsymbol{w})-y\big)\cdot \frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w}\end{align}\qquad(4)</script><h3 id="FM-模型求解（二分类问题）"><a href="#FM-模型求解（二分类问题）" class="headerlink" title="FM 模型求解（二分类问题）"></a>FM 模型求解（二分类问题）</h3><p>此时将 $\hat{y} = \pi(f(\boldsymbol{x}|\boldsymbol{w}))=\frac{1}{1+e^{-f(\boldsymbol{x}|\boldsymbol{w})}}$  作为对 $y$ 的预测结果，其中，$\pi(x)$ 为 <strong>Sigmoid</strong> 函数。还是分标签取值来进行讨论（损失函数的推导参考 <strong><a href="https://guyuecanhui.github.io/2019/05/15/lr/">LR 模型</a></strong>）。</p><h4 id="1-Label-为-1-0"><a href="#1-Label-为-1-0" class="headerlink" title="1. Label 为 {1,0}"></a>1. Label 为 {1,0}</h4><p>则将样本 $z=(\boldsymbol{x},y)$ 的损失函数定义为 <strong>LogLoss</strong> 函数：</p><script type="math/tex; mode=display">l(\boldsymbol{w},z) = -yf(\boldsymbol{x}|\boldsymbol{w})+\ln(1+e^{f(\boldsymbol{x}|\boldsymbol{w})})\big)\qquad(5)</script><p>损失函数对参数的偏导为：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial l(\boldsymbol{w},z)}{\partial w} &= -y\cdot\frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w}+\frac{1}{1+e^{f(\boldsymbol{x}|\boldsymbol{w})}}\cdot e^{f(\boldsymbol{x}|\boldsymbol{w})} \cdot\frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w} \\&=\big(\pi(f(\boldsymbol{x}|\boldsymbol{w}))-y\big)\cdot \frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w} \\&=\big(\hat{y}-y\big)\cdot \frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w}\end{align} \qquad (6)</script><h4 id="2-Label-为-1-1"><a href="#2-Label-为-1-1" class="headerlink" title="2. Label 为 {1,-1}"></a>2. Label 为 {1,-1}</h4><p>则将样本 $z=(\boldsymbol{x},y)$ 的损失函数定义为 <strong>SigmoidLoss</strong> 函数：</p><script type="math/tex; mode=display">l(\boldsymbol{w},z) = \ln(1+e^{-yf(\boldsymbol{x}|\boldsymbol{w})})\big)\qquad(7)</script><p>损失函数对参数的偏导为：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial l(\boldsymbol{w},z)}{\partial w} &= \frac{1}{1+e^{-yf(\boldsymbol{x}|\boldsymbol{w})}}\cdot e^{-yf(\boldsymbol{x}|\boldsymbol{w})} \cdot(-y)\cdot\frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w} \\&=y\cdot\big(\frac{1}{1+e^{-yf(\boldsymbol{x}|\boldsymbol{w})}}-1\big)\cdot \frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w} \\&=y\cdot\Big(\pi\big(yf(\boldsymbol{x}|\boldsymbol{w})\big)-1\Big)\cdot \frac{\partial f(\boldsymbol{x}|\boldsymbol{w})}{\partial w}\end{align} \qquad (8)</script><h2 id="二、FTRL-Optimizer-介绍"><a href="#二、FTRL-Optimizer-介绍" class="headerlink" title="二、FTRL Optimizer 介绍"></a>二、FTRL Optimizer 介绍</h2><p>上面一陀公式实际上是优化算法求梯度的时候用到的。优化算法目前有很多种，在如在线更新模型或者在线排序等对性能有严格要求的场景中，模型的稀疏解十分关键。稀疏的模型意味着只保留最关键特征的参数，意味着更少的存储、查询与计算。为了得到模型的稀疏解，通常的做法是使用 <strong>L1</strong> 正则、基于参数大小或者累积梯度大小的截断等技术。其中，<strong>FTRL</strong> 集众家之长，实现了精度与稀疏性的平衡$^{[3]}$。</p><p><strong>FTRL</strong> 更像是一种启发式的模型组装，其特征权重的更新公式为：</p><script type="math/tex; mode=display">\boldsymbol{w}^{t+1}=\arg \min_\boldsymbol{w}(\boldsymbol{g}^{1:t}\cdot \boldsymbol{w}+\lambda_1 || \boldsymbol{w}||_1+\frac{1}{2}\lambda_2 || \boldsymbol{w}||_2^2 +\frac{1}{2}\sum_{j=1}^t\sigma^j || \boldsymbol{w}-\boldsymbol{w}^j ||_2^2) \qquad(9)</script><p>其中，$\boldsymbol{g}^{1:t}$ 表示 $1\sim t$ 轮迭代中参数梯度的累积和，其中，<strong>L1</strong> 正则化部分是为了生成稀疏解，<strong>L2</strong> 正则化部分是为了使解更平滑（在论文的推导中不包含这一项），而 $\parallel \boldsymbol{w}-\boldsymbol{w}^t\parallel^2_2$ 是为了保证 $\boldsymbol{w}$ 不要离已迭代过的解太远。经过比较复杂的推导（参考文献 [4]），可以得到每一维参数的求解式：</p><script type="math/tex; mode=display">w^{t+1}_i=\begin{cases}\begin{align}0  &, \qquad |z^t_i|<\lambda_1 \\-\Big(\lambda_2+\frac{\beta+\sqrt{s^t_i}}{\alpha}\Big)^{-1}\cdot\big(z^t_i-\lambda_1\cdot \text{sgn}(z^t_i)\big) &, \qquad \text{otherwise}\end{align}\end{cases} \qquad(10)</script><p>其中，令 <script type="math/tex">z_i^t=g^{1:t}-\sum_{s=1}^t\sigma_s\boldsymbol{w}_s, \ s_i^t=\sum_{j=1}^t (g_i^j)^2</script>，主要是方便存储和迭代计算。</p><h2 id="三、基于-FTRL-训练-FM-的算法流程"><a href="#三、基于-FTRL-训练-FM-的算法流程" class="headerlink" title="三、基于 FTRL 训练 FM 的算法流程"></a>三、基于 FTRL 训练 FM 的算法流程</h2><p>用 <strong>FTRL</strong> 来训练 <strong>FM</strong> 模型，由于我们组习惯用 {0,1} 作为样本标签，则根据式 (2), (6), (10)，可以得到如下算法流程：</p><h3 id="Algorithm-Ftrl-FM"><a href="#Algorithm-Ftrl-FM" class="headerlink" title="Algorithm Ftrl+FM"></a><strong>Algorithm</strong> Ftrl+FM</h3><img src="/2019/07/03/ftrl-fm/alg-ftrl-fm.png" title="用 FTRL 算法训练 FM 模型流程"><p><strong>FTRL</strong> 算法特别适合在线更新模型，即基于每条实时样本更新模型。但是出于性能和可靠性考虑，也可以稍加修改应用于离线训练或者近线批量训练。例如，离线训练任务，将每天/每小时的数据作为一个批次用来更新 <strong>FM</strong> 模型：在每轮迭代时，需要将一批次样本的所有梯度、损失等计算结果进行汇总（可以简单的用平均值来代替），再用汇总后的值更新模型。为了训练充分，可以对每个批次的样本迭代训练若干轮。训练完的模型需要将参数 $\boldsymbol{w}, \boldsymbol{s}, \boldsymbol{z}$ 保存起来，下次加载后再增量更新；而在线预测时，只需要加载参数 $\boldsymbol{w}$ 即可。</p><p>另外，由于短视频的标签、UP 主等特征变化较快，因此对于离散特征的编码可以考虑使用特征 Hash，虽然牺牲了一定的可解释性，并且存在一定的编码冲突，但是实测下来效果还是不错的，并且工程上确实能省下很多麻烦，提升不少性能。</p><p>最后，虽然 <strong>FM</strong> 模型具备表征特征两两组合的能力，但是实际上我们发现由于样本、调参等的限制，并不能充分发掘每对特征组合的作用，并且该模型对于三个及以上特征的组合就完全无能为力了。因此，实际应用时还是不能太依赖模型的自动特征组合能力，如果有什么对业务比较有帮助的特征，还是人工生成，再一起丢到模型里去训练吧。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Rendle, S. (2011). Factorization Machines. <em>IEEE International Conference on Data Mining</em>.<br>[2] Rendle, S. (2012). Factorization machines with libfm. <em>Acm Transactions on Intelligent Systems &amp; Technology, 3</em>(3), 1-22.<br>[3] McMahan, H. B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., … &amp; Chikkerur, S. (2013, August). Ad click prediction: a view from the trenches. In <em>Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em> (pp. 1222-1230). ACM.<br>[4] 冯扬 (2014). 在线最优化求解.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近期尝试了基于 &lt;strong&gt;FTRL&lt;/strong&gt; 来训练 &lt;strong&gt;FM&lt;/strong&gt; 模型，用于短视频的排序。这篇博客主要总结一下算法的理论推导和工程化的一些心得。&lt;/p&gt;
&lt;h2 id=&quot;一、FM-Factorization-Machines-模型推
      
    
    </summary>
    
      <category term="推荐系统" scheme="https://guyuecanhui.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="优化" scheme="https://guyuecanhui.github.io/tags/%E4%BC%98%E5%8C%96/"/>
    
      <category term="FTRL" scheme="https://guyuecanhui.github.io/tags/FTRL/"/>
    
      <category term="模型" scheme="https://guyuecanhui.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="FM" scheme="https://guyuecanhui.github.io/tags/FM/"/>
    
  </entry>
  
  <entry>
    <title>二项 Logistic Regression 模型</title>
    <link href="https://guyuecanhui.github.io/2019/05/15/lr/"/>
    <id>https://guyuecanhui.github.io/2019/05/15/lr/</id>
    <published>2019-05-15T14:02:00.000Z</published>
    <updated>2019-05-18T05:44:10.328Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二项-Logistic-Regression-模型推导"><a href="#二项-Logistic-Regression-模型推导" class="headerlink" title="二项 Logistic Regression 模型推导"></a>二项 <strong>Logistic Regression</strong> 模型推导</h2><h3 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h3><p>记 $\pi(x)=\frac{1}{1+e^{-x}}$，二项 <strong>Logistic Regression</strong> 模型是如下的条件概率分布：</p><script type="math/tex; mode=display">\begin{cases}P(y=1|\boldsymbol{x})=\pi(\boldsymbol{wx})=\frac{1}{1+e^{-\boldsymbol{wx}}}=\frac{e^{\boldsymbol{wx}}}{1+e^{\boldsymbol{wx}}} \\P(y\not=1|\boldsymbol{x})=1-\pi(\boldsymbol{wx})=\frac{1}{1+e^{\boldsymbol{wx}}}\end{cases}\qquad(1)</script><h3 id="模型求解（极大似然估计）"><a href="#模型求解（极大似然估计）" class="headerlink" title="模型求解（极大似然估计）"></a>模型求解（极大似然估计）</h3><p>常见的 label 设置有正负样本分别为 {1,0} 或 {1,-1}，下面分别讨论两种设置下的损失函数和梯度的推导。首先要假设训练样本独立同分布并且数量足够，模型中待估计的参数为 $\boldsymbol{w}$，似然函数的目标是 $y_i=1$ 时 $\pi(\boldsymbol{wx}_i)$ 尽可能大，且 $y_i\not =1$ 时 $1-\pi(\boldsymbol{wx}_i)$ 尽可能大。</p><h4 id="1-label-为-1-0"><a href="#1-label-为-1-0" class="headerlink" title="1. label 为 {1,0}"></a>1. label 为 {1,0}</h4><p>此时，可以直接将 $\hat{y}=\pi(\boldsymbol{wx})$ 的结果作为对 $y$ 值的预测（或者说是预测结果为 1 的概率）。根据<a href="https://guyuecanhui.github.io/2019/05/11/terminology/">最大似然估计公式</a>，$p(\boldsymbol{x}_i|\boldsymbol{w})=\big(\pi(\boldsymbol{wx}_i)\big)^{y_i}\cdot\big(1-\pi(\boldsymbol{wx}_i)\big)^{1-y_i}$，对数似然函数可以设计为：</p><script type="math/tex; mode=display">\begin{align}H(\boldsymbol{w}) &=\arg \max_{\boldsymbol{w}}\sum_{i=1}^N \ln\Big(\big(\pi(\boldsymbol{wx}_i)\big)^{y_i}\cdot\big(1-\pi(\boldsymbol{wx}_i)\big)^{1-y_i}\Big) \\&=\arg \max_{\boldsymbol{w}}\sum_{i=1}^N \Big(y_i\cdot \ln\big(\pi(\boldsymbol{wx}_i)\big)+(1-y_i)\cdot\big(1-\pi(\boldsymbol{wx}_i)\big)\Big) \\&=\arg \max_{\boldsymbol{w}}\sum_{i=1}^N \Big(y_i\cdot \ln(\frac{e^{\boldsymbol{wx}_i}}{1+e^{\boldsymbol{wx}_i}})+(1-y_i)\cdot \ln(\frac{1}{1+e^{\boldsymbol{wx}_i}})\Big) \\&=\arg \max_{\boldsymbol{w}}\sum_{i=1}^N \Big(y_i\boldsymbol{wx}_i-\ln(1+e^{\boldsymbol{wx}_i})\Big)\qquad(2)\end{align}</script><p>这里，$l_l(\boldsymbol{x},y)=-\big(y\cdot \ln(\hat{y})+(1-y)\cdot\ln(1-\hat{y})\big)=\ln(1+e^{f(\boldsymbol{x})})-yf(\boldsymbol{x})$ 记作样本 $ (\boldsymbol{x},y)$ 的 <strong>LogLoss</strong>，后面会经常见到。</p><p>根据损失函数 $l_l(\boldsymbol{x},y)$，对每个维度上的参数分别求导：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial l_l(\boldsymbol{x},y)}{\partial w_i}&=\frac{1}{(1+e^{\boldsymbol{wx}})}\cdot e^{\boldsymbol{wx}}\cdot x_i-y\cdot x_i\\&=\big(\pi(\boldsymbol{wx})-y\big)\cdot x_i\\&=(\hat{y}-y)\cdot x_i\qquad(3)\end{align}</script><h4 id="2-label-为-1-1"><a href="#2-label-为-1-1" class="headerlink" title="2. label 为 {1,-1}"></a>2. label 为 {1,-1}</h4><p>此时仍然可以认为 $\pi(\boldsymbol{wx})$ 输出了模型预测样本结果为 1 的概率，但是由于负样本的标签为 -1，因此考虑使用 $p(\boldsymbol{x}_i|\boldsymbol{w})=\frac{1}{1+e^{-y_i\boldsymbol{wx}_i}}$，则对数似然函数可以设计为：</p><script type="math/tex; mode=display">\begin{align}H(\boldsymbol{w}) &=\arg \max_{\boldsymbol{w}}\sum_{i=1}^N \ln(\frac{1}{1+e^{-y_i\boldsymbol{wx}_i}}) \\&=\arg \min_{\boldsymbol{w}}\sum_{i=1}^N \ln(1+e^{-y_i\boldsymbol{wx}_i}) \qquad(4)\\\end{align}</script><p>这里，$l_s(\boldsymbol{x},y)=\ln(1+e^{-y\boldsymbol{wx}})$ 称作样本 $(\boldsymbol{x},y)$ 的 <strong>SigmoidLoss</strong>，后面也会经常看到。</p><p>根据损失函数 $l_s(\boldsymbol{x},y)$，对每个维度上的参数分别求导：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial l_s(\boldsymbol{x},y)}{\partial w_i}&=\frac{1}{1+e^{-y\boldsymbol{wx}}}\cdot e^{-y\boldsymbol{wx}}\cdot (-y\cdot x_i) \\&=y\cdot \big(\pi(y\boldsymbol{wx})-1\big)\cdot x_i\qquad(4)\\\end{align}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;二项-Logistic-Regression-模型推导&quot;&gt;&lt;a href=&quot;#二项-Logistic-Regression-模型推导&quot; class=&quot;headerlink&quot; title=&quot;二项 Logistic Regression 模型推导&quot;&gt;&lt;/a&gt;二项 &lt;s
      
    
    </summary>
    
      <category term="数学" scheme="https://guyuecanhui.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="模型" scheme="https://guyuecanhui.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="线性模型" scheme="https://guyuecanhui.github.io/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="LR" scheme="https://guyuecanhui.github.io/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>符号约定与常用公式</title>
    <link href="https://guyuecanhui.github.io/2019/05/11/terminology/"/>
    <id>https://guyuecanhui.github.io/2019/05/11/terminology/</id>
    <published>2019-05-11T13:51:23.000Z</published>
    <updated>2019-05-18T05:43:43.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、推荐系统常用符号含义"><a href="#一、推荐系统常用符号含义" class="headerlink" title="一、推荐系统常用符号含义"></a>一、推荐系统常用符号含义</h2><div class="table-container"><table><thead><tr><th style="text-align:left">符号表示</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:left">$\boldsymbol{x}$</td><td style="text-align:left">输入变量，一般为特征向量</td></tr><tr><td style="text-align:left">$\boldsymbol{x}_i=(x_i^{(1)}, \cdots, x_i^{(n)})^{\top}$</td><td style="text-align:left">第 $i$ 个输入变量的取值，在推导损失函数等场景下，由于每次只考虑一条样本，记样本为 $\boldsymbol{x}=(x_1,\cdots,x_n)$，此时 $x_i$ 表示样本的第 $i$ 维特征</td></tr><tr><td style="text-align:left"><script type="math/tex">\mathcal{X}=\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\}</script></td><td style="text-align:left">输入实例集合</td></tr><tr><td style="text-align:left"><script type="math/tex">(x_j^{(i)})^k</script></td><td style="text-align:left">第 $j$ 个输入变量的第 $i$ 维特征取值的 $k$ 次方</td></tr><tr><td style="text-align:left">$y$</td><td style="text-align:left">输出变量，一般为样本标签</td></tr><tr><td style="text-align:left">$y_i$</td><td style="text-align:left">第 $i$ 个输出变量的取值</td></tr><tr><td style="text-align:left">$\mathcal{Y}={y_1,\cdots,y_N}$</td><td style="text-align:left">输出实例集合</td></tr><tr><td style="text-align:left">$(\boldsymbol{x}_i,y_i)$</td><td style="text-align:left">第 $i$ 个样本点</td></tr><tr><td style="text-align:left">$\mathcal{T}={(\boldsymbol{x}_1,y_1),\cdots,(\boldsymbol{x}_N,y_N)}$</td><td style="text-align:left">训练数据集</td></tr><tr><td style="text-align:left">$\boldsymbol{w}=(w_1,\cdots,w_n)$</td><td style="text-align:left">权重向量</td></tr><tr><td style="text-align:left">$w_i^t$</td><td style="text-align:left">第 $i$ 维特征的权重在第 $t$ 轮迭代的取值</td></tr><tr><td style="text-align:left">$\parallel \boldsymbol{w} \parallel_i^j$</td><td style="text-align:left">权重向量 $\boldsymbol{w}$ 的 Li 范数的 $j$ 次方，例如 L1 范数：$\parallel \boldsymbol{w} \parallel_1$，L2 范数： $\parallel \boldsymbol{w} \parallel_2^2$</td></tr><tr><td style="text-align:left">$\boldsymbol{g}=(g_1,\cdots,g_n)$</td><td style="text-align:left">梯度向量</td></tr><tr><td style="text-align:left">$\psi(\boldsymbol{w})$</td><td style="text-align:left">正则化函数</td></tr></tbody></table></div><h2 id="二、常用定理"><a href="#二、常用定理" class="headerlink" title="二、常用定理"></a>二、常用定理</h2><h3 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><ol><li>样本的平均值约等于总体的平均值。</li><li>给定一个任意分布的总体，从中随机抽取 $N$ 个样本，抽取 $k$ 次，这 $k$ 组抽样平均值的分布接近正态分布。</li><li>经验表明，当每组抽样数量 $N\ge 30$ 时就服从中心极限定理。</li></ol><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p><strong>前提假设：</strong>训练样本的分布能代表样本的真实分布；每个样本集中的样本都是所谓独立同分布的随机变量，且有充分的训练样本。</p><p><strong>最大似然估计的目的是：</strong>利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：<strong>模型已定，参数未知</strong>。</p><p>ML估计的求解方法：</p><script type="math/tex; mode=display">\hat{\theta} = \arg \max_{\theta} l(\theta) = \arg \max_{\theta}\prod_{i=1}^N p(\boldsymbol{x}_i|\theta)</script><p>为了便于分析，定义对数似然函数 $H(\theta) = \ln l(\theta)$，则：</p><script type="math/tex; mode=display">\hat{\theta} = \arg \max_{\theta} \ln l(\theta) = \arg \max_{\theta}\sum_{i=1}^N \ln p(\boldsymbol{x}_i|\theta)</script><p>当 $H(\theta)$ 连续可微的情况下，可以通过求导（单个未知参数）或者求梯度（多个未知参数）的方式求解方程。</p><h2 id="三、常用的函数和公式"><a href="#三、常用的函数和公式" class="headerlink" title="三、常用的函数和公式"></a>三、常用的函数和公式</h2><h3 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h3><ul><li>表达式：$\pi(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{1+e^x}$；</li><li>导数：$\pi’(x)=\pi(x)\big(1-\pi(x)\big)$；</li></ul><h3 id="LogLoss"><a href="#LogLoss" class="headerlink" title="LogLoss"></a>LogLoss</h3><ul><li>样本的 $(\boldsymbol{x},y)$ 的 <strong>SigmoidLoss</strong> 表达式：$l_{l}(\boldsymbol{x},y)=\ln(1+e^{f(\boldsymbol{x})})-yf(\boldsymbol{x})$</li><li>导数：$l_l’(\boldsymbol{x},y)=\Big(\pi\big(f(\boldsymbol{x})\big)-y\Big)\cdot f’(\boldsymbol{x})$</li><li>使用极大似然估计，标签值为 {0,1}，推导参考 <a href="https://guyuecanhui.github.io/2019/05/15/lr/">LR 模型</a></li></ul><h3 id="SigmoidLoss"><a href="#SigmoidLoss" class="headerlink" title="SigmoidLoss"></a>SigmoidLoss</h3><ul><li>样本的 $(\boldsymbol{x},y)$ 的 <strong>SigmoidLoss</strong> 表达式：$l_s(\boldsymbol{x},y)=\ln(1+e^{-yf(\boldsymbol{x})})$</li><li>导数：$l_s’(\boldsymbol{x},y)=y\Big(\pi\big(y\cdot f(\boldsymbol{x})\big)-1\Big)\cdot f’(\boldsymbol{x})$</li><li>使用极大似然估计，标签值为 {-1,1}，推导参考 <a href="https://guyuecanhui.github.io/2019/05/15/lr/">LR 模型</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、推荐系统常用符号含义&quot;&gt;&lt;a href=&quot;#一、推荐系统常用符号含义&quot; class=&quot;headerlink&quot; title=&quot;一、推荐系统常用符号含义&quot;&gt;&lt;/a&gt;一、推荐系统常用符号含义&lt;/h2&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;ta
      
    
    </summary>
    
      <category term="数学" scheme="https://guyuecanhui.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="符号" scheme="https://guyuecanhui.github.io/tags/%E7%AC%A6%E5%8F%B7/"/>
    
      <category term="定理" scheme="https://guyuecanhui.github.io/tags/%E5%AE%9A%E7%90%86/"/>
    
      <category term="公式" scheme="https://guyuecanhui.github.io/tags/%E5%85%AC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>从 SimRank 到 SimRank++</title>
    <link href="https://guyuecanhui.github.io/2019/05/10/simrankpp/"/>
    <id>https://guyuecanhui.github.io/2019/05/10/simrankpp/</id>
    <published>2019-05-10T14:42:18.000Z</published>
    <updated>2019-05-12T14:48:49.603Z</updated>
    
    <content type="html"><![CDATA[<h2 id="从-SimRank-到-SimRank"><a href="#从-SimRank-到-SimRank" class="headerlink" title="从 SimRank 到 SimRank++"></a>从 SimRank 到 SimRank++</h2><p>上一篇博客<a href="https://guyuecanhui.github.io/2019/04/29/simrank/">《SimRank与视频相似度计算》</a> 介绍了 <strong>SimRank</strong>$^{[1]}$ 及其在视频推荐中的应用，这一篇再谈谈 <strong>SimRank++</strong>。顾名思义，<strong>SimRank++</strong> 是在 <strong>SimRank</strong> 的基础上做了一些优化，在文献 [2] 中提出时是为了解决搜索词改写的问题，本质上也就是计算搜索词的相似度。作者发现，当需要考虑二部图的边权信息时，原始的 <strong>SimRank</strong> 模型难以评估物品间相似度的可信度，这篇博客从视频推荐的角度来阐释作者的优化点。</p><h3 id="从用户到用户群"><a href="#从用户到用户群" class="headerlink" title="从用户到用户群"></a>从用户到用户群</h3><p>由于我们影片的观众量级在千万级，而影片的数量在十万级，因此使用 <strong>SimRank</strong> 模型来计算视频相似度时，最大的计算和存储瓶颈在于<code>用户相似矩阵</code>、<code>用户-视频转移矩阵</code>及<code>视频-用户转移矩阵</code>。但是从使用场景上来讲，我们在这里实际上并不需要度量用户之间的相似度（尽管它们可以用来做用户协同推荐），用户仅仅是用来传递视频间的相似度。因此，为了减少计算和存储的开销，我们可以对用户进行聚类，使用用户组来代替用户完成视频相似度的传递。</p><p>基于这个想法，我们可以使用各种聚类的方法：按用户性别、年龄、地域等；我是直接基于历史行为进行用户聚类。具体的做法是基于用户最近 $N$ 天的播放、收藏、分享等行为生成用户的表征向量（可以用 AutoEncoder、PCA 等方法），然后基于表征向量执行 KMeans（直接 KMeans 可能跑不出来），这里的用户群数量需要根据实际场景调试，我们希望类内最大距离越小越好。然后再将用户的行为聚合到用户组，例如有效播放次数累加、总播放时长的累加、总播放占比的累加、平均 CTR 等。这样我们就从<code>用户-视频二部图</code>切换到了<code>用户组-视频</code>二部图，整个网络的规模降低了 2 个数量级。</p><h3 id="SimRank-的优化"><a href="#SimRank-的优化" class="headerlink" title="SimRank++ 的优化"></a>SimRank++ 的优化</h3><p>我们将用户聚成了用户组，丢失了大量的网络信息，虽然用户组作为网络中的一个节点，我们看不出它的出边是来自哪些用户，但是好在我们保留了这个组里有多少用户看了某个视频。而由于这一组用户又是相似的，因此我们期望通过充分利用边权来最小化网络信息的丢失。</p><p><strong>SimRank++</strong> 正好满足我们的需求，它在 <strong>SimRank</strong> 的基础上增加了两项优化：</p><h4 id="1-两个节点共同节点越多，则这两个节点相似度的可信度越高"><a href="#1-两个节点共同节点越多，则这两个节点相似度的可信度越高" class="headerlink" title="1. 两个节点共同节点越多，则这两个节点相似度的可信度越高"></a>1. 两个节点共同节点越多，则这两个节点相似度的可信度越高</h4><p>这一条很容易理解，如果很多人同时看了两部视频，那这两部视频的相似度也就越可信（注意，共同观看越多并不意味着相似度越高）。例如下图所示，视频 $v_1,v_2$ 与 $v_3,v_4$ 相比同时被更多的用户组同时观看，因此 $v_1,v_2$ 根据 <strong>SimRank</strong> 模型算出来的相似度应该比 $v_3,v_4$ 的相似度更可信。</p><img src="/2019/05/10/simrankpp/group-watch-1.png" title="共同节点越多，相似度可信度越高"><p>我们用 $E(i,j)$ 来表示节点 $i,j$ 相似度的可信度，论文 [2] 中推荐使用 $E<em>v(v_i,v_j)=\sum</em>{k=1}^{|I(v_i)\cap I(v_j)|} \frac{1}{2^k}$ 或者 $E_v(v_i,v_j)=1-e^{|I(v_i)\cap I(v_j)|}$ 来评估该权重（用户侧的同理），则：</p><script type="math/tex; mode=display">\begin{cases}s(u,u')=c_1\cdot E_u(u,u')\cdot \sum_{i\in O(u)}\sum_{j\in O(u')}W_{uv}(u,i)\cdot W_{uv}(u',j)\cdot s(i, j) \\s(v,v')=c_2\cdot E_v(v,v')\cdot \sum_{i\in I(v)}\sum_{j\in I(v')}W_{vu}(v,i)\cdot W_{vu}(v',j)\cdot s(i, j)\end{cases} \qquad (1)</script><h4 id="2-节点边权越大、差异越小，则它的邻居节点相似度的权重越高"><a href="#2-节点边权越大、差异越小，则它的邻居节点相似度的权重越高" class="headerlink" title="2. 节点边权越大、差异越小，则它的邻居节点相似度的权重越高"></a>2. 节点边权越大、差异越小，则它的邻居节点相似度的权重越高</h4><p>如下图所示，我们用用户播放数来表示边权（如上所述，并非只有这一种权重表示方法）。不考虑边权时，$s(v_1,v_2)$ 和 $s(v_3,v_4)$ 完全相同。但是实际上，由于用户组 1 中有 100 个人看了 $v_1$ 和 $v_2$，可以认为用户组 1 中很多人都同时喜欢 $v_1,v_2$；而用户组 2 中有 100 个人看了 $v_3$，但只有 1 个人看了 $v_4$，因此 $v_3$ 和 $v_4$ 显然相似度应该比 $v_1,v_2$ 的低。</p><img src="/2019/05/10/simrankpp/group-watch-2.png" title="边权越大、差异越小，相似度越高"><p>我们用新的权重 $P(i,j)$ 来表示节点 $i,j$ 的相似度传导权重，则：</p><script type="math/tex; mode=display">P(i,j)=e^{-var(j)}\frac{w(i,j)}{\sum_{k\in N(i)}w(i,k)}\quad (2)</script><p>其中，$e^{-var(i)}$ 用来度量节点 $i$ 的边权差异，边权差异越大，该系数越小；$\frac{w(i,j)}{\sum_{k\in N(i)}w(i,k)}$ 则是用来计算归一化的权重。</p><h3 id="SimRank-模型的矩阵描述"><a href="#SimRank-模型的矩阵描述" class="headerlink" title="SimRank++ 模型的矩阵描述"></a>SimRank++ 模型的矩阵描述</h3><p>基于式 (1) 和 式 (2)，我们可以写出 <strong>SimRank++</strong> 的矩阵描述：</p><script type="math/tex; mode=display">\begin{cases}S_u^{k+1} = c_1\cdot E_u\circ P_{vu}^T\cdot S_v^k \cdot P_{vu} + (I - diag(c_1\cdot E_u\circ P_{vu}^T\cdot S_v^k \cdot P_{vu})) \\S_v^{k+1} = c_2\cdot E_v\circ P_{uv}^T\cdot S_u^k \cdot P_{uv} + (I-diag(c_2\cdot E_v\circ P_{uv}^T\cdot S_u^k \cdot P_{uv}))\end{cases}\quad (3)</script><p>其中，</p><script type="math/tex; mode=display">\begin{cases}E_u(u,u')=1-e^{-|O(u)\cap O(u')|}\\E_v(v,v')=1-e^{-|I(v)\cap I(v')|}\end{cases}\quad (4)</script><script type="math/tex; mode=display">\begin{cases}P_{uv}(u,v)=e^{-var(v)}\frac{w(u,v)}{\sum_{i\in O(u)}w(u,i)} \\P_{vu}(v,u)=e^{-var(u)}\frac{w(v,u)}{\sum_{i\in I(v)}w(v,i)}\end{cases} \qquad(5)</script><p>但是使用数据进行验证时，发现该模型对用户聚类的效果和权重的设置十分敏感，这两项没调好的话，很容易导致算出来的视频相似列表趋同或者有其他的问题。具体来说，用户聚类的原则是类内用户的行为越相似越好；权重的话则没有很明显的规律，需要根据业务场景来尝试了。</p><h3 id="发散讨论：扩散算法"><a href="#发散讨论：扩散算法" class="headerlink" title="发散讨论：扩散算法"></a>发散讨论：扩散算法</h3><p>前几天跟其他同学交流的时候，有人提过之前做过用热传导算法来算用户的个性化推荐结果，据说效果也很不错。这里顺便扒一扒热传导算法和 <strong>SimRank</strong> 算法的区别和联系。</p><p>首先，它们都属于扩散算法，都是基于对物理世界现象的观察和模拟。典型的扩散有两类：一类是物质或者能量的扩散，满足守恒律，常称作为<strong>物质扩散</strong>，最终稳定下来后，总量是不变的；另一类是热的扩散，一般由一个或多个恒温热源驱动，不满足守恒律，常被称作为<strong>热传导</strong>。SimRank 是属于热传导（物品与自己的相似度恒定为 1）。</p><p>相比而言，物质扩散倾向于推荐比较流行的物品，而热传导倾向于推荐比较冷门的物品。更详细的讨论可以参考文献 [3]。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Jeh, G., &amp; Widom, J. (2002, July). SimRank: a measure of structural-context similarity. In <em>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</em> (pp. 538-543). ACM.</p><p>[2] Antonellis, I., Molina, H. G., &amp; Chang, C. C. (2008). Simrank++: query rewriting through link analysis of the click graph. <em>Proceedings of the VLDB Endowment</em>, <em>1</em>(1), 408-421.</p><p>[3] 推荐算法整理 — 扩散算法. <em><a href="https://www.zybuluo.com/chanvee/note/21053" target="_blank" rel="noopener">https://www.zybuluo.com/chanvee/note/21053</a>.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;从-SimRank-到-SimRank&quot;&gt;&lt;a href=&quot;#从-SimRank-到-SimRank&quot; class=&quot;headerlink&quot; title=&quot;从 SimRank 到 SimRank++&quot;&gt;&lt;/a&gt;从 SimRank 到 SimRank++&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="推荐系统" scheme="https://guyuecanhui.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="协同过滤" scheme="https://guyuecanhui.github.io/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
      <category term="算法" scheme="https://guyuecanhui.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="相似度" scheme="https://guyuecanhui.github.io/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
      <category term="二部图" scheme="https://guyuecanhui.github.io/tags/%E4%BA%8C%E9%83%A8%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>SimRank与视频相似度计算</title>
    <link href="https://guyuecanhui.github.io/2019/04/29/simrank/"/>
    <id>https://guyuecanhui.github.io/2019/04/29/simrank/</id>
    <published>2019-04-29T01:09:50.000Z</published>
    <updated>2019-05-10T14:44:49.781Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、应用背景"><a href="#一、应用背景" class="headerlink" title="一、应用背景"></a>一、应用背景</h2><p>最近需要对视频的相关推荐进行一些优化。之前尝试过 TagSim、AutoEncoder 和 Word2Vec 等方法，无非是基于元数据相似或基于协同相似的思路。但是在实际应用的时候，由于媒资传过来的信息未必是非常准确的，因此基于元数据相似的方法在数据基础上可能就存在一定的不确定性，因此常常会推出来一些虽然实际上很符合算法预期，但是看起来很奇怪的结果。而基于协同相似的推荐，由于需要比较多的行为数据来估计视频之间的相似度，又往往只能覆盖少量的视频。在应用中，我们往往使用的是两者的混合，但是由于混合比较简单粗暴，仍然有很多 VOC 问题。</p><p>因此，团队迫切的需要一种能够提升相关推荐效果的模型。而这种相关又是有强业务语义的，需要能够支持灵活的定制，因此在短时间内先不考虑深度网络（可解释性太差）。在调研中，发现有基于热传导的算法，感觉好像挺符合直观感觉，用了协同数据，同时也支持元数据。但是再顺着这个思路往下找的时候，发现  <strong>SimRank</strong> 是一种十分成熟且常用于相关推荐的模型，粗看了一下，感觉很符合我们的业务诉求，就迫不及待尝试了一下。</p><h2 id="二、SimRank-基本模型"><a href="#二、SimRank-基本模型" class="headerlink" title="二、SimRank 基本模型"></a>二、SimRank 基本模型</h2><h3 id="2-1-核心思想"><a href="#2-1-核心思想" class="headerlink" title="2.1 核心思想"></a>2.1 核心思想</h3><p>由于 <strong>SimRank</strong> 提出的时间比较早，网上的材料很多，而且大多长的也差不多，可以参考文献 [1, 2] ，这里只简单的搬个砖。</p><p>文献 [1] 最早提出  <strong>SimRank</strong> 模型，核心的思想是 “<strong>two objects are similar if they are related to similar objects</strong>“（这跟 PageRank 的思路完全一致，只是 PageRank 是用来评估每个链接的重要性，而  <strong>SimRank</strong> 是用来评估每两个物品间的相似度）。 <strong>SimRank</strong> 既支持计算所有节点对之间的相似度（如输入数据为文章引用记录），也支持计算二部图中每一部分节点间的相似度（如输入数据为用户行为记录）。由于我们是做视频推荐，主要用的是用户行为数据，因此这里只介绍基于二部图的模型。</p><p>举个简单的例子：如下图所示，用户 $u_1$ 观看了视频 $v_1,v_2,v_3$；用户 $u_2$ 观看了视频 $v_2,v_3,v_4$，则可以用二部图来表示这种观影关系（二部图是因为用户 $u_1,u_2$ 之间无联系，且视频 $v_1,v_2,v_3,v_4$ 间无联系，只有用户-视频间存在有向边）：</p><img src="/2019/04/29/simrank/simrank-f1.png" title="用户观影二部图示例"><p>为了评估视频 $v_1,v_4$ 之间的相似度，需要看看哪些人看了 $v_1,v_4$ ，以及这些用户的相似度。这是一个典型的递归逻辑，递归的起点在于：每个节点（包括这里的用户/视频）与自己的相似度为 1；没有关联的节点间相似度为 0（一种情况是这两个节点没有与其他节点的联系，还有一种情况是在迭代的初始状态时，所有节点对间的相似度为 0）。值得注意的是，如果用 ItemCF 算法来计算 $v_1,v_4$ 的相似度，由于它们没有共同观看的用户，相似度为 0，具体对比可以参考我之前的博客：<a href="https://guyuecanhui.github.io/2019/04/12/itemcf/">可能是最好懂的ItemCF解释了</a>。</p><h3 id="2-2-基于二部图的描述"><a href="#2-2-基于二部图的描述" class="headerlink" title="2.2 基于二部图的描述"></a>2.2 基于二部图的描述</h3><p>最直观和容易理解的是基于图的描述。用数学语言来表达上面的思路：</p><script type="math/tex; mode=display">\begin{cases}s(u,u')=\frac{c_1}{|O(u)|\cdot|O(u')|}\sum_{i\in O(u)}\sum_{j\in O(u')}s(i, j) \\s(v,v')=\frac{c_2}{|I(v)|\cdot|I(v')|}\sum_{i\in I(v)}\sum_{j\in I(v')}s(i, j)\end{cases} \quad (1)</script><p>其中，$u$ 表示用户，$v$ 表示视频，$O(u)$ 表示用户 $u$ 观看过的视频集合，$I(v)$ 表示视频的观看用户集合，$s(i,j)$ 表示两个节点的相似度，$c_i$ 为常数系数。式 (1) 中累加相似度的部分不是很好理解，实际上就是对两个节点所有关联的节点进行两两组合计算相似度之和。$c_1, c_2$ 可以理解成相似度的传导率，传导率越大，受到相邻节点影响也就越大，每轮迭代相似度的传播也就越快，表现为迭代若干轮后，节点间的相似度越高（文献 [1] 中建议的是0.8）。如果使用随机游走的方法，则传导率越大，下一个状态转移到相邻节点的概率越大，即下一个状态保持原来节点概率越小。</p><p>在实现模型的时候，可以直接在图上按公式  (1)  进行计算，但是需要注意缓存中间结果$^{[3]}$，否则存在很多重复计算，实测中，不做什么优化的话，超过 $10000\times10000$ 的二部图单机基本就几个小时都算不出来了。</p><h3 id="2-3-基于矩阵的描述"><a href="#2-3-基于矩阵的描述" class="headerlink" title="2.3 基于矩阵的描述"></a>2.3 基于矩阵的描述</h3><p>另外一种等价的描述是将图转化成矩阵，比如原来的二部图是 $G = (U, V, E)$，即共 $n_u + n_v$ 个节点，可以转化成 $(n_u + n_v) \times (n_u + n_v)$ 的状态转移矩阵 $W$。根据公式 (1) 的描述，图中的每一条边对应于转移矩阵的一个元素（这里实现的时候用户和视频一般是分开连续编号的），从而可以设置转移矩阵为： </p><script type="math/tex; mode=display">\begin{cases}w(u,v)=\frac{1}{|O(u)|} \\w(v,u)=\frac{1}{|I(v)|}\end{cases}</script><p>转移矩阵中其他元素为 0。而根据定义，相似度矩阵 $S$ 中对角线始终为 1，其他元素初始化为 0。则基于矩阵的迭代过程可以用下式来表达：</p><script type="math/tex; mode=display">S = C\cdot W^T\cdot S\cdot W + (I-diag(C\cdot W^T\cdot S\cdot W)) \quad (2)</script><p>其中，矩阵 $C$ 的对角线元素为 <script type="math/tex">c_1</script> 或 <script type="math/tex">c_2</script>，如果 <script type="math/tex">c_1=c_2=c</script>，那 $C$ 可以直接用系数 $c$ 来代替。公式 (2) 的前一部分就是公式 (1) 的矩阵描述，后一部分实际上是为了设置每轮迭代时，相似矩阵的对角线为 1，即 <script type="math/tex">s_{i,i}=1</script>。</p><p>注意到，在二部图的情况下，<code>用户-视频</code>的相似度必然是 0，同时，<code>用户-用户</code> / <code>视频-视频</code>的转移矩阵也必然是 0。因此相似矩阵和转移矩阵可以简单的拆成<code>用户-用户</code>相似矩阵 <script type="math/tex">S_u</script>、<code>视频-视频</code> <script type="math/tex">S_v</script> 相似矩阵以及<code>用户-视频</code>转移矩阵 <script type="math/tex">W_{uv}</script>、<code>视频-用户</code>转移矩阵 $W_{vu}$，并做分块乘法。简单的推导一下：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}W^T\cdot S\cdot W &=\left[\begin{array}{cc}0 & W_{vu}^T \\W_{uv}^T & 0\end{array}\right] \cdot\left[\begin{array}{cc}S_u & 0 \\0 & S_v\end{array}\right] \cdot\left[\begin{array}{cc}0 & W_{uv} \\W_{vu} & 0\end{array}\right] \\&=\left[\begin{array}{cc}0 & W_{vu}^T\cdot S_v \cdot W_{vu} \\W_{uv}^T\cdot S_u \cdot W_{uv} & 0\end{array}\right]\end{aligned}\end{equation}</script><p>仔细品味一下这个公式，能更直观的了解相似度的传递过程。因此，迭代计算公式为：</p><script type="math/tex; mode=display">\begin{cases}S_u^{k+1} = c_1 \cdot W_{vu}^T\cdot S_v^k \cdot W_{vu} + (I - diag(c_1 \cdot W_{vu}^T\cdot S_v^k \cdot W_{vu})) \\S_v^{k+1} = c_2 \cdot W_{uv}^T\cdot S_u^k \cdot W_{uv} + (I-diag(c_2\cdot W_{uv}^T\cdot S_u^k \cdot W_{uv}))\end{cases}\quad (3)</script><h3 id="2-4-扩展用户-视频属性"><a href="#2-4-扩展用户-视频属性" class="headerlink" title="2.4 扩展用户/视频属性"></a>2.4 扩展用户/视频属性</h3><p>以上描述了经典的基于二部图的  <strong>SimRank</strong> 算法，但是其实我们可以将视频的元数据/用户的属性数据作为辅助节点加入到图中来，并添加<code>视频元数据</code>$\rightarrow$<code>视频</code>和<code>用户画像</code>$\rightarrow$<code>用户</code>的单向边（表示用户/视频的相似度不会反向传播给画像/元数据），同时初始化不同维度的视频元数据/用户画像的相似度，以达到运营干预的目的。具体的分块乘法就不推导了，跟 2.3 节差不多，这里只举一个例子：</p><img src="/2019/04/29/simrank/simrank-f2.png" title="扩展用户/视频属性后的二部图示例"><p>上图中，本来 $u_1,u_2$ 之间是没有边相连的，因此相似度为 0，但是由于他们同属男性，因此由<code>男性</code>这个画像向这两个用户传播了一定的相似度；同样的，本来 $v_1,v_2$ 之间的相似度也为 0，但是由于它们都是搞笑的视频，因此<code>搞笑</code>这个元数据也向它们传播了一定的相似度。<strong>加入用户维度和视频维度的辅助节点以后，有助于解决由于行为较少而无法准确评估相似度的情况。</strong></p><h2 id="三、模型实现与优化讨论"><a href="#三、模型实现与优化讨论" class="headerlink" title="三、模型实现与优化讨论"></a>三、模型实现与优化讨论</h2><p>我在 spark 2 和在 python 中分别实现了上述过程，使用图遍历的方式优点是代码简单，但是对于大规模的图优化比较麻烦，速度很慢；使用矩阵计算的时候，主要的问题又在于矩阵的优化计算。下面简单讲下一些可行的优化思路。</p><h5 id="a-基于-spark-的精确计算"><a href="#a-基于-spark-的精确计算" class="headerlink" title="a. 基于 spark 的精确计算"></a>a. 基于 spark 的精确计算</h5><p>如果使用 mllib 的 <code>BlockMatrix</code> 来计算，会强制将稀疏矩阵转成稠密矩阵来计算，因此开销比实际需要的大很多，因此一定要使用公式 (3) 来替代公式 (2)。但是即使这样，也不能从根本上解决问题，根本上是需要自己实现一套高效的分布式稀疏矩阵的计算方法，网上有一些开源项目可参考。</p><h5 id="b-基于-python-的精确计算"><a href="#b-基于-python-的精确计算" class="headerlink" title="b. 基于 python 的精确计算"></a>b. 基于 python 的精确计算</h5><p>使用 python 进行计算时，由于相似度的精度要求不高，因此使用 <code>np.float16</code> 就足够了，并且每轮迭代完，要将小于一定阈值的相似项置 0（如果只需要计算 topN相似的话，每轮可以只保留系数最大的 topN 项）。另外，构建矩阵用 <code>csr_matrix</code> 比较方便，计算的时候还是得用 <code>li_matrix</code>。</p><h5 id="c-迭代近似"><a href="#c-迭代近似" class="headerlink" title="c. 迭代近似"></a>c. 迭代近似</h5><p>由于我们只需要算视频的相似度，有一种解决上面问题的思路是将用户随机分成若干份，用这些用户的数据来计算视频的相似矩阵，然后将这些相似矩阵加起来求平均，但是效果不是很好。</p><h5 id="d-矩阵分析"><a href="#d-矩阵分析" class="headerlink" title="d. 矩阵分析"></a>d. 矩阵分析</h5><p>针对矩阵运算，可以预先分析矩阵的特点，然后再采用一定的手段来减少总计算量。这里涉及一些矩阵分解的优化方法，以后有机会再仔细研究研究。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Jeh, G., &amp; Widom, J. (2002, July). SimRank: a measure of structural-context similarity. In <em>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</em> (pp. 538-543). ACM.</p><p>[2] SimRank协同过滤推荐算法: <a href="http://www.cnblogs.com/pinard/p/6362647.html" target="_blank" rel="noopener">http://www.cnblogs.com/pinard/p/6362647.html</a>.</p><p>[3] Lizorkin, D., Velikhov, P., Grinev, M., &amp; Turdakov, D. (2008). Accuracy estimate and optimization techniques for simrank computation. <em>Proceedings of the VLDB Endowment</em>, <em>1</em>(1), 422-433.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、应用背景&quot;&gt;&lt;a href=&quot;#一、应用背景&quot; class=&quot;headerlink&quot; title=&quot;一、应用背景&quot;&gt;&lt;/a&gt;一、应用背景&lt;/h2&gt;&lt;p&gt;最近需要对视频的相关推荐进行一些优化。之前尝试过 TagSim、AutoEncoder 和 Word2Vec
      
    
    </summary>
    
      <category term="推荐系统" scheme="https://guyuecanhui.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="协同过滤" scheme="https://guyuecanhui.github.io/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
      <category term="算法" scheme="https://guyuecanhui.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="相似度" scheme="https://guyuecanhui.github.io/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
      <category term="二部图" scheme="https://guyuecanhui.github.io/tags/%E4%BA%8C%E9%83%A8%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+NexT+github 配置指南</title>
    <link href="https://guyuecanhui.github.io/2019/04/14/hexo-next-github/"/>
    <id>https://guyuecanhui.github.io/2019/04/14/hexo-next-github/</id>
    <published>2019-04-14T06:43:50.000Z</published>
    <updated>2019-04-14T14:02:49.468Z</updated>
    
    <content type="html"><![CDATA[<p>这两天在网络各位大神的帖子指导下完成了 Hexo+Next 在 github 上的部署，记录一下全过程，以供后来者参考。</p><p>进入正题前，需要安装 Node.js 和 Git，并创建 github 帐号，可以参考其他帖子，这里就不详述了。</p><h2 id="安装使用-Hexo"><a href="#安装使用-Hexo" class="headerlink" title="安装使用 Hexo"></a>安装使用 Hexo</h2><h3 id="安装-Hexo"><a href="#安装-Hexo" class="headerlink" title="安装 Hexo"></a>安装 Hexo</h3><p>Hexo 安装过程很简单，只需要选择一个项目的名称和目录（如 <code>project</code>），然后输入以下代码即可在当前目录创建一个 <code>project</code> 的目录，同时初始化该工程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br><span class="line">hexo init project</span><br><span class="line">cd project</span><br><span class="line">npm install</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>如果打开 <code>[http://localhost:4000](http://localhost:4000/)</code> 能够看到 <code>Hello World</code>，说明安装已经成功了。</p><h3 id="Hexo-常用命令"><a href="#Hexo-常用命令" class="headerlink" title="Hexo 常用命令"></a>Hexo 常用命令</h3><p>Hexo 常用的命令不多，而且很容易记，按一般使用顺序记录如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo n "new post" # 创建新博客，名为 new post</span><br><span class="line">hexo g # 生成静态文件</span><br><span class="line">hexo s # 在本地起一个服务，可以查看效果，默认路径为 http://localhost:4000</span><br><span class="line">hexo d # 将本地文件部署到远端，本文将使用 github 托管</span><br></pre></td></tr></table></figure><h2 id="部署到-github"><a href="#部署到-github" class="headerlink" title="部署到 github"></a>部署到 github</h2><h3 id="创建新仓库"><a href="#创建新仓库" class="headerlink" title="创建新仓库"></a>创建新仓库</h3><p>首先需要创建一个新的仓库，例如我的用户名为 <code>guyuecanhui</code>，则我需要创建的仓库名为 <code>guyuecanhui.github.io</code>。注意：仓库的名字一定要按照规范命名，即 <code>用户名.github.io</code>，否则无法通过该路径访问博客。</p><h3 id="配置-Hexo-的-deploy-选项"><a href="#配置-Hexo-的-deploy-选项" class="headerlink" title="配置 Hexo 的 deploy 选项"></a>配置 Hexo 的 deploy 选项</h3><p>配置 <code>project</code> 根目录下的 <code>_config.yml</code>，在最后添加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: http://github.com/guyuecanhui/guyuecanhui.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>注意仓库的路径按照参考代码设置，把用户名改成自己的即可。</p><p>配置完后执行（这两步后面简称重新部署）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>即可把本地的博客发布到 github 上了。可以通过 <code>用户名.github.io</code> 来访问主页了。</p><h2 id="安装和配置-NexT-主题"><a href="#安装和配置-NexT-主题" class="headerlink" title="安装和配置 NexT 主题"></a>安装和配置 NexT 主题</h2><p>我使用 Hexo 的目的之一是使用网络大神们提供的各种养眼的主题。这里推荐安装 NexT 主题，也是当前使用比较广泛的一种。</p><h3 id="下载并使用-NexT-主题"><a href="#下载并使用-NexT-主题" class="headerlink" title="下载并使用 NexT 主题"></a><a href="https://theme-next.org/docs/getting-started/" target="_blank" rel="noopener">下载并使用 NexT 主题</a></h3><p>首先定位到博客根目录下，然后下载 NexT 源码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>然后修改根目录下的 <code>_config.yml</code> 的 <code>theme</code> 选项，修改为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure><p>然后重新部署博客即可看到主题变成了 NexT。NextT 提供了四种模式，在 <code>themes/next/_config.yml</code> 中可以修改，每次修改后需要重新部署才能生效：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment">#scheme: Muse   # 默认风格</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="comment">#scheme: Pisces</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Gemini</span>  <span class="comment"># 选择 gemini 风格</span></span><br></pre></td></tr></table></figure><h3 id="常用选项设置"><a href="#常用选项设置" class="headerlink" title="常用选项设置"></a>常用选项设置</h3><p>只要记住，跟主题相关的所有设置基本都可以在 <code>themes/next/_config.yml</code> 里面找到。</p><h4 id="访问统计"><a href="#访问统计" class="headerlink" title="访问统计"></a><a href="http://ibruce.info/2015/04/04/busuanzi/" target="_blank" rel="noopener">访问统计</a></h4><p>使用不蒜子统计博客访问的人数，首先要引入 <code>busuanzi.js</code>。先在 <code>themes/next/_config.yml</code> 中启用并配置 <code>busuanzi_count</code> 插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">busuanzi_count:</span><br><span class="line">  <span class="comment"># count values only if the other configs are false</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># custom uv span for the whole site</span></span><br><span class="line">  site_uv: <span class="literal">true</span></span><br><span class="line">  site_uv_header: &lt;i class=<span class="string">"fa fa-user"</span>&gt;&lt;/i&gt; 访问人数</span><br><span class="line">  site_uv_footer: 人</span><br><span class="line">  <span class="comment"># custom pv span for the whole site</span></span><br><span class="line">  site_pv: <span class="literal">true</span></span><br><span class="line">  site_pv_header: &lt;i class=<span class="string">"fa fa-eye"</span>&gt;&lt;/i&gt; 总访问量</span><br><span class="line">  site_pv_footer: 次</span><br><span class="line">  <span class="comment"># custom pv span for one page only</span></span><br><span class="line">  page_pv: <span class="literal">true</span></span><br><span class="line">  page_pv_header: &lt;i class=<span class="string">"fa fa-file-o"</span>&gt;&lt;/i&gt; 阅读数</span><br><span class="line">  page_pv_footer:</span><br></pre></td></tr></table></figure><p>然后在 <code>themes/next/layout/_partial/footer.swig</code> 中添加以下代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">src</span>=<span class="string">"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>重新部署后，就能看到博客的访问统计数据啦！</p><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ul><li><a href="https://blog.csdn.net/qq_31640513/article/details/83931014" target="_blank" rel="noopener">Latex 支持</a></li><li><a href="https://www.jianshu.com/p/e17711e44e00" target="_blank" rel="noopener">添加分类、标签页面</a></li><li><a href="https://www.jianshu.com/p/60f5c84aa5a8" target="_blank" rel="noopener">添加关于页面</a></li><li><a href="https://www.jianshu.com/p/cf0628478a4e" target="_blank" rel="noopener">添加图片支持</a></li><li><a href="https://blog.csdn.net/blue_zy/article/details/79071414" target="_blank" rel="noopener">添加评论</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这两天在网络各位大神的帖子指导下完成了 Hexo+Next 在 github 上的部署，记录一下全过程，以供后来者参考。&lt;/p&gt;
&lt;p&gt;进入正题前，需要安装 Node.js 和 Git，并创建 github 帐号，可以参考其他帖子，这里就不详述了。&lt;/p&gt;
&lt;h2 id=&quot;
      
    
    </summary>
    
      <category term="安装部署" scheme="https://guyuecanhui.github.io/categories/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
      <category term="hexo" scheme="https://guyuecanhui.github.io/tags/hexo/"/>
    
      <category term="github" scheme="https://guyuecanhui.github.io/tags/github/"/>
    
      <category term="next" scheme="https://guyuecanhui.github.io/tags/next/"/>
    
  </entry>
  
  <entry>
    <title>可能是最好懂的ItemCF解释了</title>
    <link href="https://guyuecanhui.github.io/2019/04/12/itemcf/"/>
    <id>https://guyuecanhui.github.io/2019/04/12/itemcf/</id>
    <published>2019-04-12T05:56:07.000Z</published>
    <updated>2019-05-04T13:33:58.679Z</updated>
    
    <content type="html"><![CDATA[<p>说到推荐系统，可能最为人熟知的算法就是协同过滤，特别是其中的 ItemCF，自亚马逊文章发表以后，得到了广泛而成功的应用。这篇文章主要谈谈我的理解。</p><h2 id="ItemCF-推导过程"><a href="#ItemCF-推导过程" class="headerlink" title="ItemCF 推导过程"></a>ItemCF 推导过程</h2><p>首先，ItemCF 依赖一个隐含的假设：就是每个用户的兴趣都局限在某几个方面，因此如果两个物品属于一个用户的兴趣列表，那么这两个物品可能就属于有限的几个领域，而如果两个物品属于很多用户的兴趣列表，那么它们就可能属于同一个领域，因而有很大的相似度。</p><p>从这个假设出发，我们可以认为两个视频相似表现在它们被很多用户同时观看，两个物品共同观看的人数越多，说明它们的相似度越高，用公式来表达就是：</p><script type="math/tex; mode=display">s_{i,j}=|N(i)\cap N(j)| \qquad (1)</script><p>其中，$N(i)$ 表示观看视频 $i$ 的人群集合，$|N(i)\cap N(j)|$ 表示同时播放过 $i,j$ 的人数。但是由于热点视频可能并不代表用户的真实兴趣（有可能是运营推送，或者仅仅是由于追热心理），因此需要惩罚那些热点的视频，可以通过将共同观看人数除以与视频总观看数相关的系数来实现，例如使用以下方式：</p><script type="math/tex; mode=display">s_{i,j}=\frac{|N(i)\cap N(j)|}{\sqrt{|N(i)|\cdot|N(j)|}} \qquad (2)</script><p>但是仅仅惩罚热点视频也还不够，有些人就是闲的无聊，有什么看什么，这种情况下他表现出来的就未必是真实的兴趣了（就不满足我们的隐含假设），他的行为也就不太能作为我们的协同的依据，因此需要对这种人做降权，例如使用以下方式：</p><script type="math/tex; mode=display">s_{i,j}=\frac{\sum_{u\in N(i)\cap N(j)}\frac{1}{log(1+|M(u)|)}}{\sqrt{|N(i)|\cdot|N(j)|}} \qquad (3)</script><p>其中，$M(u)$ 表示用户 $u$ 观看的视频集合。另外，如果用户对视频 $i$ 观看了 80%，而对视频 $j$ 只看了 10%，那用户对这两个视频的喜欢程度也是不相同的，因此我们还可以对用户对两个视频观看的完成度差异做降权，差异越大，相似度也越低，例如使用以下方式：</p><script type="math/tex; mode=display">s_{i,j}=\frac{\sum_{u\in N(i)\cap N(j)}\frac{\cos(r_u(i),r_u(j))}{log(1+|M(u)|)}}{\sqrt{|N(i)|\cdot|N(j)|}} \qquad (4)</script><p>其中，$r_u(i)$ 表示用户 $u$ 对视频 $i$ 的观看完成度。最后，将所有视频与其他视频的相似度做 $max$ 归一化，得到：</p><script type="math/tex; mode=display">s_{i,j}'=\frac{s_{i,j}}{\max_j s_{i,j}} \qquad (5)</script><p>归一化使得所有物品的相似度取值都在 (0,1] 之间，这个相似度已经可以直接用于相关推荐场景。另外，有研究表明，这种归一化可以提高 ItemCF 用于个性化推荐时的准确度、覆盖率和多样性。</p><p>那基于 ItemCF 如何进行个性化推荐呢？主要是考虑推荐与用户的观看历史最相似的视频，即计算每个视频与用户观看视频集合的相似度作为作为是否观看该视频的预测值：</p><script type="math/tex; mode=display">p_u(i)=\frac{\sum_{j\in M(u)} s_{i,j}\cdot r_u(j)}{\sum_{j\in M(u)} s_{i,j}} \qquad (6)</script><p>最后，再根据预测值从大到小选取 TopN 个视频作为推荐结果。</p><h2 id="优化与讨论"><a href="#优化与讨论" class="headerlink" title="优化与讨论"></a>优化与讨论</h2><p>其实从第 1 部分的介绍来看，基于 ItemCF 的思想可以做很多改进。例如：</p><ol><li>如果感觉算出来的结果仍然偏向热门视频时，可以增加式 (4) 的分母大小；</li><li>如果觉得用户只有观看完完成度很高时才是真实兴趣，那可以将式 (4) 的 $cos(\cdot)$ 部分改成类似 $r_u(i)\cdot r_u(j)$ 的形式；</li><li>如果觉得需要更多的考虑用户的短期兴趣，做即时的推荐，那可以将式 (6) 中的用户观看历史限制在最近几次，甚至一次；</li></ol><p>如果把用户-视频考虑成一个二部图，ItemCF 实际上是基于图的结构，执行了一次从用户到视频的兴趣扩散过程。可以考虑下图中的视频 $v_1,v_3$，它们没有直接的共同观看，因此用 ItemCF 算出来的相似度为 0，但是实际上 $u_1,u_2$ 都观看了 $v_2$，因此可以认为用户 $u_1,u_2$ 存在一定的相似性，因此如果执行一次视频-用户-视频的兴趣扩散过程就能够捕获 $v_1,v_3$ 的相似度了。</p><img src="/2019/04/12/itemcf/itemcf-f1.png" title="用户观影行为示例"><p>后面一种思路实际上就是 SimRank，但是由于需要执行多次兴趣扩散（即对二部图做多次迭代计算），SimRank 的计算复杂度相当高，在业务数据量大的情况下需要强大的算力支持，以后会再讨论下我在 SimRank 模型上的尝试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说到推荐系统，可能最为人熟知的算法就是协同过滤，特别是其中的 ItemCF，自亚马逊文章发表以后，得到了广泛而成功的应用。这篇文章主要谈谈我的理解。&lt;/p&gt;
&lt;h2 id=&quot;ItemCF-推导过程&quot;&gt;&lt;a href=&quot;#ItemCF-推导过程&quot; class=&quot;headerl
      
    
    </summary>
    
      <category term="推荐系统" scheme="https://guyuecanhui.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐" scheme="https://guyuecanhui.github.io/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="协同过滤" scheme="https://guyuecanhui.github.io/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
      <category term="算法" scheme="https://guyuecanhui.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
